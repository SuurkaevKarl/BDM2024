{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004ad8f-9f34-485a-aeb6-1c111b04eb2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Config for Delta\n",
    "\n",
    "The next two cells are likely not needed as the setup of the project's template seems to indicate that we don't need to use Delta Lake(s). Then again, Idk what I'm on about. -Karl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793c7f6d-bb47-4d32-b329-7bc5390a1adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, python-dotenv, delta-spark\n",
      "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d54c8d-f142-40ef-bc4f-728564cf6baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Project 2 - Streaming\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\")\\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca98bc-2c83-4a90-8cc4-8fae463ce67a",
   "metadata": {},
   "source": [
    "## Template continues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "# TO MODIFY FOR YOUR SCHEMA\n",
    "# Modified schema\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), False),\n",
    "    StructField(\"hack_license\", StringType(), False),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), False),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), False),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), False),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69712d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I added both ports because some StackOverflow threads recommended it as a fix, did not seem to affect much\n",
    "#   -Karl\n",
    "kafka_server = \"kafka1:9092,kafka2:9093\"\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                          # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"taxi_rides\")               # Subscribe to the \"taxi_rides\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()                                          # Load the DataFrame\n",
    ")\n",
    "\n",
    "# Convert the value column to string\n",
    "value_df = lines.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON and create DataFrame\n",
    "parsed_df = value_df.select(from_json(col(\"value\"), schema).alias(\"parsed_value\"))\n",
    "\n",
    "# Select the parsed fields\n",
    "df = parsed_df.select(\"parsed_value.*\")\n",
    "\n",
    "df.printSchema()  # To check if the schema is correctly applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "## The project starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc431b49-0040-4d01-ab99-b9fe6b916927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col, udf, date_format\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2523093-bb5e-4917-9f30-f7f6d94b3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())\n",
    "\n",
    "# Create method for creating table - taken from 06_Streaming2/Streaming2.ipynb practice materials\n",
    "# NB! Seems to be related to Delta Lakes, likely not needed\n",
    "def create_table_if_exists(output_path,table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60): # you can replace this with while, currently timeouts after about 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\"))>0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{table_name}'\") # table metastore is created once there is some data (.parquet) in the directory\n",
    "                break\n",
    "        except Exception as e:\n",
    "            #print(e) # if you want to see the exceptions, uncomment this\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting terminated streams.. done!\n",
      "List of active streams: [<pyspark.sql.streaming.query.StreamingQuery object at 0x7f344aab3ed0>]\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[taxi_rides]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Terminated with exception: Failed to construct kafka consumer', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = ae03cc79-55eb-4807-9416-f5bb37356391, runId = 36353140-f855-42a5-8717-ea4d0777c690] terminated with exception: Failed to construct kafka consumer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Helps with debugging sometimes\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = ae03cc79-55eb-4807-9416-f5bb37356391, runId = 36353140-f855-42a5-8717-ea4d0777c690] terminated with exception: Failed to construct kafka consumer"
     ]
    }
   ],
   "source": [
    "# These variables would be used if following the Delta Lake examples\n",
    "# But they can also be repurposed for other uses so it's OK to keep them\n",
    "table_name = \"query1\"\n",
    "checkpoint_path = f\"streaming/{table_name}/_checkpoint\" \n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "# Set up window conf for comparing with previous fares\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_datetime\"))\n",
    "\n",
    "# Unsure if this bit is necessary, but might be useful for when using .awaitTermination()\n",
    "# See more here: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination.html#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination\n",
    "print(\"Resetting terminated streams.. \", end=\"\")\n",
    "spark.streams.resetTerminated()\n",
    "print(\"done!\")\n",
    "\n",
    "# All the commented out lines are different experimentations I have tried for reading and/or displaying data from Kafka\n",
    "# The current uncommented lines are the closest I got to getting at least some sort of result from Kafka\n",
    "#   -Karl\n",
    "query = (df\n",
    "           #.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "           #.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000)\n",
    "           #.withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "           #.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "           #.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))\n",
    "           #.withWatermark(col(\"pickup_datetime\"), \"2 hours\")\n",
    "           #.withWatermark(col(\"timestamp\"), \"2 hours\")\n",
    "           .select(col(\"timestamp\"))\n",
    "           #.groupBy(col(\"medallion\")).agg(sum(col(\"timestamp\")))\n",
    "           .writeStream\n",
    "           .outputMode(\"append\")\n",
    "           #.outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "           #.format(\"delta\")\n",
    "           .format(\"console\") # This should make the output appear in the console\n",
    "           #.queryName(table_name)\n",
    "           #.trigger(processingTime=\"5 second\")\n",
    "           #.option(\"checkpointLocation\", checkpoint_path)\n",
    "           #.start(output_path)\n",
    "           .start()\n",
    ")\n",
    "\n",
    "# Debugging for seeing what the status of the query is\n",
    "print(\"List of active streams: \" + str(spark.streams.active))\n",
    "n = 0\n",
    "while n < 1000:\n",
    "    print(query.status)\n",
    "    if \"Terminated\" in query.status.get(\"message\"):\n",
    "        break\n",
    "    n += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# Helps with debugging sometimes\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
