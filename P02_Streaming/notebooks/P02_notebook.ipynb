{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004ad8f-9f34-485a-aeb6-1c111b04eb2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Config for Delta\n",
    "\n",
    "The next two cells are likely not needed as the setup of the project's template seems to indicate that we don't need to use Delta Lake(s). Then again, Idk what I'm on about. -Karl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793c7f6d-bb47-4d32-b329-7bc5390a1adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, python-dotenv, delta-spark\n",
      "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d54c8d-f142-40ef-bc4f-728564cf6baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Project 2 - Streaming\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\")\\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca98bc-2c83-4a90-8cc4-8fae463ce67a",
   "metadata": {},
   "source": [
    "## Template continues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "# TO MODIFY FOR YOUR SCHEMA\n",
    "# Modified schema\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), False),\n",
    "    StructField(\"hack_license\", StringType(), False),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), False),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), False),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), False),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69712d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"taxi_rides\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "# Load the DataFrame\n",
    ")\n",
    "\n",
    "# Convert the value column to string\n",
    "value_df = lines.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON and create DataFrame\n",
    "parsed_df = value_df.select(from_json(col(\"value\"), schema).alias(\"parsed_value\"))\n",
    "\n",
    "# Select the parsed fields\n",
    "df = parsed_df.select(\"parsed_value.*\")\n",
    "\n",
    "df.printSchema()  # To check if the schema is correctly applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "## The project starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc431b49-0040-4d01-ab99-b9fe6b916927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col, udf, date_format\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2523093-bb5e-4917-9f30-f7f6d94b3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())\n",
    "\n",
    "# Create method for creating table - taken from 06_Streaming2/Streaming2.ipynb practice materials\n",
    "def create_table_if_exists(output_path,table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60): # you can replace this with while, currently timeouts after about 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\"))>0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{table_name}'\") # table metastore is created once there is some data (.parquet) in the directory\n",
    "                break\n",
    "        except Exception as e:\n",
    "            #print(e) # if you want to see the exceptions, uncomment this\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `medallion` cannot be resolved. Did you mean one of the following? [`timestamp`].;\n'Aggregate ['medallion], ['medallion, sum(cast(timestamp#33 as double)) AS sum(timestamp)#321]\n+- Project [timestamp#33]\n   +- Project [parsed_value#23.medallion AS medallion#25, parsed_value#23.hack_license AS hack_license#26, parsed_value#23.pickup_datetime AS pickup_datetime#27, parsed_value#23.dropoff_datetime AS dropoff_datetime#28, parsed_value#23.pickup_longitude AS pickup_longitude#29, parsed_value#23.pickup_latitude AS pickup_latitude#30, parsed_value#23.dropoff_longitude AS dropoff_longitude#31, parsed_value#23.dropoff_latitude AS dropoff_latitude#32, parsed_value#23.timestamp AS timestamp#33]\n      +- Project [from_json(StructField(medallion,StringType,false), StructField(hack_license,StringType,false), StructField(pickup_datetime,TimestampType,false), StructField(dropoff_datetime,TimestampType,false), StructField(pickup_longitude,DoubleType,false), StructField(pickup_latitude,DoubleType,false), StructField(dropoff_longitude,DoubleType,false), StructField(dropoff_latitude,DoubleType,false), StructField(timestamp,TimestampType,false), value#21, Some(Etc/UTC)) AS parsed_value#23]\n         +- Project [cast(value#8 as string) AS value#21]\n            +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@38f64dd6, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7502c6c8, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=taxi_rides, maxOffsetsPerTrigger=100], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@45ed96e8,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> taxi_rides, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set up window conf for comparing with previous fares\u001b[39;00m\n\u001b[1;32m      6\u001b[0m window_conf \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedallion\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m test_df \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000)\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withWatermark(col(\"pickup_datetime\"), \"2 hours\")\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#.withWatermark(col(\"timestamp\"), \"2 hours\")\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedallion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m            \u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m     19\u001b[0m            \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m            \u001b[38;5;66;03m#.outputMode(\"complete\") # we overwrite the complete table with every trigger\u001b[39;00m\n\u001b[1;32m     21\u001b[0m            \u001b[38;5;66;03m#.format(\"delta\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m            \u001b[38;5;241m.\u001b[39mqueryName(table_name)\n\u001b[1;32m     23\u001b[0m            \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 second\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m            \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpoint_path)\n\u001b[1;32m     25\u001b[0m            \u001b[38;5;241m.\u001b[39mstart(output_path)\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `medallion` cannot be resolved. Did you mean one of the following? [`timestamp`].;\n'Aggregate ['medallion], ['medallion, sum(cast(timestamp#33 as double)) AS sum(timestamp)#321]\n+- Project [timestamp#33]\n   +- Project [parsed_value#23.medallion AS medallion#25, parsed_value#23.hack_license AS hack_license#26, parsed_value#23.pickup_datetime AS pickup_datetime#27, parsed_value#23.dropoff_datetime AS dropoff_datetime#28, parsed_value#23.pickup_longitude AS pickup_longitude#29, parsed_value#23.pickup_latitude AS pickup_latitude#30, parsed_value#23.dropoff_longitude AS dropoff_longitude#31, parsed_value#23.dropoff_latitude AS dropoff_latitude#32, parsed_value#23.timestamp AS timestamp#33]\n      +- Project [from_json(StructField(medallion,StringType,false), StructField(hack_license,StringType,false), StructField(pickup_datetime,TimestampType,false), StructField(dropoff_datetime,TimestampType,false), StructField(pickup_longitude,DoubleType,false), StructField(pickup_latitude,DoubleType,false), StructField(dropoff_longitude,DoubleType,false), StructField(dropoff_latitude,DoubleType,false), StructField(timestamp,TimestampType,false), value#21, Some(Etc/UTC)) AS parsed_value#23]\n         +- Project [cast(value#8 as string) AS value#21]\n            +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@38f64dd6, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7502c6c8, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=taxi_rides, maxOffsetsPerTrigger=100], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@45ed96e8,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> taxi_rides, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "table_name = \"query1\"\n",
    "checkpoint_path = f\"streaming/{table_name}/_checkpoint\" \n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "# Set up window conf for comparing with previous fares\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_datetime\"))\n",
    "\n",
    "test_df = (df\n",
    "           #.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "           #.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000)\n",
    "           #.withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "           #.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "           #.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))\n",
    "           #.withWatermark(col(\"pickup_datetime\"), \"2 hours\")\n",
    "           #.withWatermark(col(\"timestamp\"), \"2 hours\")\n",
    "           .select(col(\"timestamp\"))\n",
    "           .groupBy(col(\"medallion\")).agg(sum(col(\"timestamp\")))\n",
    "           .writeStream\n",
    "           .outputMode(\"append\")\n",
    "           #.outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "           #.format(\"delta\")\n",
    "           .queryName(table_name)\n",
    "           .trigger(processingTime=\"5 second\")\n",
    "           .option(\"checkpointLocation\", checkpoint_path)\n",
    "           .start(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
