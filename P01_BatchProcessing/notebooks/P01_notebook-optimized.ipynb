{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e3377-3e6f-4877-bcb3-a77adf2a3dec",
   "metadata": {},
   "source": [
    "# 00 - Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bec91-f82b-46de-a342-05e9abe4bdc3",
   "metadata": {},
   "source": [
    "## 00.a. Unzip CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98c96d-a863-43fc-8bc6-d41e6c45e613",
   "metadata": {},
   "source": [
    "1. Unzip the 7z file. Manual task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc0fc9-56df-4b14-a1fc-175c5d07ad32",
   "metadata": {},
   "source": [
    "### 00.b - Create dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc22297-0906-434a-a62b-48b0507708ed",
   "metadata": {},
   "source": [
    "Can be skipped if directories already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42739bf-f734-4126-bf1d-451767d3979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already exists\n",
      "data/in already exists\n",
      "data/out already exists\n",
      "data/out/table already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "dirs = [\"data\",\"data/in\",\"data/out\",\"data/out/table\"]\n",
    "\n",
    "for directory in dirs:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} already exists\")\n",
    "    else:\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7319c-aa8f-42c4-9b36-621625a1a953",
   "metadata": {},
   "source": [
    "### 00.c - Run this to cleanup/reset the out folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf75ec0-7287-41d7-b7f6-00f9e9f8c578",
   "metadata": {},
   "source": [
    "Can be skipped if some of the query execution is to be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b15be1a9-56cb-488a-8f0e-fdd25a8e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/table/enrich_trip_data_table does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "file_name=\"trip_data\"\n",
    "raw_table_name=\"raw_trip_data_table\"\n",
    "enrich_table_name=\"enrich_trip_data_table\"\n",
    "\n",
    "parq_output_files=[\"data/out/\"+file_name+\".parquet\",\n",
    "                  \"data/out/table/\"+raw_table_name,\n",
    "                  \"data/out/table/\"+enrich_table_name]\n",
    "\n",
    "# Check if the directory exists\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81f844-0502-4026-94d2-601cc368ac56",
   "metadata": {},
   "source": [
    "## Reset Enrichment Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c85ba64c-7c9e-42a3-a65e-e83af7173bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/table/enrich_trip_data_table has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "parq_output_files=[\"data/out/table/\"+enrich_table_name]\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02794cd1-e87e-4fc3-874f-9407ada3b17e",
   "metadata": {},
   "source": [
    "### 00.d - Install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48fb3f1a-4fac-4321-8807-abb5db79679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.0)\n",
      "Requirement already satisfied: install in /opt/conda/lib/python3.11/site-packages (1.3.5)\n",
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.11/site-packages (0.14.4)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (24.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip install geopandas shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c4752-8dcf-4ce6-8f00-73ac0fde3dbc",
   "metadata": {},
   "source": [
    "# IF ALL PREREQUISITES ARE SATISFIED, THE PROCESS STARTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44cda8-2acf-4c4a-839c-5ff4ad90215f",
   "metadata": {},
   "source": [
    "### 00.d - Set common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0c2a8b4-4703-47ed-a63d-391977b77022",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"trip_data\"\n",
    "raw_table_name=\"raw_trip_data_table\"\n",
    "enrich_table_name=\"enrich_trip_data_table\"\n",
    "# Query tables\n",
    "query1_table_name=\"query1_idle_time\"\n",
    "query2_table_name=\"query2_avg_idle_time_per_borough\"\n",
    "query3_table_name=\"query3_trip_same_borough\"\n",
    "query4_table_name=\"query4_trip_diff_borough\"\n",
    "\n",
    "column_partitioner=\"medallion\"\n",
    "bucket_count=400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a10fa5-c8a6-47ad-86ae-d2b8dc435cb2",
   "metadata": {},
   "source": [
    "## 00.e - Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f485d492-3fb5-4283-8a56-4acde482bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This was run on a 32gb machine. Adjust the driver memory as per laptop configuration\n",
    "# Utilizing more than 80% of the memory allocated causes error.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM2024 - Project01\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\")\\\n",
    "    .config(\"spark.memory.fraction\", \".8\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65349cf-f842-46a6-90e8-8f508537e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable below if you are resetting the docker image too.\n",
    "# This prouces a gazillion logs and you will scratch your head why are you losing so much disk space\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7737-6021-44ff-9e5f-67e3906280bb",
   "metadata": {},
   "source": [
    "# 01 - Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae344-64c0-4f39-81e9-71dcd7c191a0",
   "metadata": {},
   "source": [
    "## 1.1 Load csv files to hive table. Let's call it Raw layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82495462-bbfa-4ee0-a178-5c502e5c19a1",
   "metadata": {},
   "source": [
    "Raw Layer contains the base format of data. We only add bucketing and partitioning to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478f6c-5ac6-4676-8325-0ade01b91fce",
   "metadata": {},
   "source": [
    "### 1.1.1 Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414927a3-c2c3-4fc6-a7aa-bc55b77a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151654f-df66-41cc-ac2c-6f3f1b771603",
   "metadata": {},
   "source": [
    "### 1.1.2. List the files to be ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d9c702-19c9-49c1-b3d2-5bac3da5e4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"trip_data\"\n",
    "file_prefix=\"data/in/\"+file_name+\"_\"\n",
    "# List of CSV files\n",
    "#csv_file_paths = [file_prefix +\"test1.csv\",file_prefix +\"test2.csv\"]\n",
    "csv_file_paths = [file_prefix + \"1.csv\" , file_prefix + \"2.csv\", file_prefix + \"3.csv\",\n",
    "                  file_prefix + \"4.csv\", file_prefix + \"5.csv\", file_prefix + \"6.csv\",\n",
    "                  file_prefix + \"7.csv\", file_prefix + \"8.csv\", file_prefix + \"9.csv\",\n",
    "                  file_prefix + \"10.csv\", file_prefix + \"11.csv\", file_prefix + \"12.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d31c5-d0ab-44ee-919b-dd3796088df3",
   "metadata": {},
   "source": [
    "### 1.1.3. Declare the csv file schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b453d9-8ce6-47c9-a081-1edce72c25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "raw_file_schema= StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfae9b7-df07-4825-8d58-778d02aa40e5",
   "metadata": {},
   "source": [
    "### 1.1.4 Read the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3a2dc-f685-459f-bab3-b01e5832349f",
   "metadata": {},
   "source": [
    "Add the persists so that the file remains to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6000182-32d6-4941-bfb1-952ad1ab184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_file_paths, header=True, schema=raw_file_schema).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d283f3b-2ebd-42cc-894f-f21b5f40762c",
   "metadata": {},
   "source": [
    "### 1.1.5. Create the hive table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed86530-8ceb-4f7e-b1b0-c53ddbdcad50",
   "metadata": {},
   "source": [
    "Apply `bucketing` and `partitioning` to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf3f0b1-a183-4b43-afa6-e313050fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parquet file successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To reset the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+raw_table_name)\n",
    "df.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\\\n",
    "    .write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .partitionBy(\"pickup_date\")\\\n",
    "    .sortBy(\"pickup_datetime\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(raw_table_name)\n",
    "df.unpersist()    \n",
    "print(\"Writing parquet file successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49345af0-51dd-4782-a3cb-949851ee17c6",
   "metadata": {},
   "source": [
    "### 1.1.6 Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67652ff8-d040-449e-b9c2-e24b26b3197d",
   "metadata": {},
   "source": [
    "Check the details from the Raw Layer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29cad1f-f76f-4480-960c-9e9dd7f272fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|default  |raw_trip_data_table|false      |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+----------------------+\n",
      "|partition             |\n",
      "+----------------------+\n",
      "|pickup_date=2013-01-01|\n",
      "|pickup_date=2013-01-02|\n",
      "|pickup_date=2013-01-03|\n",
      "|pickup_date=2013-01-04|\n",
      "|pickup_date=2013-01-05|\n",
      "|pickup_date=2013-01-06|\n",
      "|pickup_date=2013-01-07|\n",
      "|pickup_date=2013-01-08|\n",
      "|pickup_date=2013-01-09|\n",
      "|pickup_date=2013-01-10|\n",
      "|pickup_date=2013-01-11|\n",
      "|pickup_date=2013-01-12|\n",
      "|pickup_date=2013-01-13|\n",
      "|pickup_date=2013-01-14|\n",
      "|pickup_date=2013-01-15|\n",
      "|pickup_date=2013-01-16|\n",
      "|pickup_date=2013-01-17|\n",
      "|pickup_date=2013-01-18|\n",
      "|pickup_date=2013-01-19|\n",
      "|pickup_date=2013-01-20|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "spark.sql(\"SHOW PARTITIONS \"+raw_table_name).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b302719-100e-4df0-9761-df4758e1a4e1",
   "metadata": {},
   "source": [
    "## 1.2 Create the enrichment layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5f2bd-12b7-4b4f-b558-8fb268e7686c",
   "metadata": {},
   "source": [
    "Enrichment layer limits the data to what is necessary to answer the queries. It also adds any needed column for the queries so that the query does minimal computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406674c-3a9c-4da5-9aa0-53e1e9f1ee3a",
   "metadata": {},
   "source": [
    "### 1.2.1. Read geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0e2789-84bb-4ae5-9034-57a3a8ea0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, LongType, ArrayType\n",
    "\n",
    "# https://gist.github.com/JulesBelveze/a552e8c53dfd1f46948cbeb32c096611\n",
    "geojson_data=\"data/in/nyc-boroughs.geojson\"\n",
    "# Define the custom schema\n",
    "geojson_schema = StructType([\n",
    "    StructField(\"type\", StringType(), nullable=True),\n",
    "    StructField(\"id\", LongType(), nullable=True),\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"boroughCode\", LongType(), nullable=True),\n",
    "        StructField(\"borough\", StringType(), nullable=True),\n",
    "        StructField(\"@id\", StringType(), nullable=True)\n",
    "    ]), nullable=True),\n",
    "    StructField(\"geometry\", StructType([\n",
    "        StructField(\"type\", StringType(), nullable=True),\n",
    "        StructField(\"coordinates\", ArrayType(ArrayType(ArrayType(DoubleType()))), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Read GeoJSON file with custom schema\n",
    "geojson_df = spark.read.schema(geojson_schema).json(geojson_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3229129-de14-4980-ae0a-2250c509c8cd",
   "metadata": {},
   "source": [
    "### 1.2.2. Sanity check for geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c73af24-a7b2-4eb7-bde6-7044ffac468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "+-------+----+--------------------+--------------------+\n",
      "|   type|  id|          properties|            geometry|\n",
      "+-------+----+--------------------+--------------------+\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+---+--------------------+--------------------+\n",
      "|   type| id|          properties|            geometry|\n",
      "+-------+---+--------------------+--------------------+\n",
      "|Feature|  0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  9|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 10|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 11|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 12|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 13|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 14|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 15|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 16|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 17|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 18|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 19|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of boroughs: 104\n"
     ]
    }
   ],
   "source": [
    "# View the schema\n",
    "geojson_df.printSchema()\n",
    "\n",
    "# Show the data\n",
    "geojson_df.show()\n",
    "geojson_df = geojson_df.na.drop() #remove the NULL values\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "geojson_df.show()\n",
    "\n",
    "num_rows = geojson_df.count()\n",
    "print(f\"Number of boroughs: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307d3cd-e636-43f0-be42-eec3be52bc0d",
   "metadata": {},
   "source": [
    "### 1.2.4. Define needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fefbb854-e840-44a7-b53b-2eee605a0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col,expr\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "def array_to_polygon(coordinates):\n",
    "    polygon = shape({\"type\": \"Polygon\", \"coordinates\": coordinates})\n",
    "    return polygon\n",
    "\n",
    "def polygon_area(coordinates):\n",
    "    polygon = shape({\"type\": \"Polygon\", \"coordinates\": coordinates})\n",
    "    return polygon.area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcbec94-6ad2-4fe9-b851-93252f7012be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.array_to_polygon(coordinates)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"array_to_polygon_udf\",array_to_polygon,StructType([StructField(\"type\", StringType()), StructField(\"coordinates\", StringType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7508bdae-8da0-43e4-b6fe-8202ae70e755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.polygon_area(coordinates)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"polygon_area_udf\",polygon_area,DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa0afd0-6e8e-4f3c-a40b-85b56bc225c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- area: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enrich the GeoJSON DataFrame with the Shapely polygons\n",
    "enriched_geojson = geojson_df.withColumn(\"area\", expr(\"polygon_area_udf(geometry.coordinates)\"))\n",
    "enriched_geojson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5853021c-4e06-48ed-a710-6c1d99f4ef39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(lat, lon)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "## here\n",
    "geometry_df = geojson_df.select(\n",
    "    \"id\",\n",
    "    col(\"properties.boroughCode\").alias(\"borough_code\"),\n",
    "    col(\"properties.borough\").alias(\"borough\"),\n",
    "    col(\"geometry.coordinates\").alias(\"coordinates\")\n",
    ")\n",
    "geojson_iterable = enriched_geojson.collect()\n",
    "\n",
    "# Define a UDF to get the borough name\n",
    "def get_borough_name(lat, lon, data):\n",
    "    point = Point(lon, lat)\n",
    "    for row in data:\n",
    "        polygon = shape({\"type\": \"Polygon\", \"coordinates\": row[\"geometry\"][\"coordinates\"]})\n",
    "        if polygon.contains(point):\n",
    "            return row[\"properties\"][\"borough\"]\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "#get_borough_name_udf = udf(lambda lat, lon: get_borough_name(lat, lon, geojson_iterable), StringType())\n",
    "spark.udf.register(\"get_borough_name_udf\",lambda lat, lon: get_borough_name(lat, lon, geojson_iterable),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67613856-d5ae-48ef-aab1-a0212751d07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(lat, lon)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "geojson_iterable = enriched_geojson.collect()\n",
    "\n",
    "# Define a UDF to get the borough name\n",
    "def get_borough_name(lat, lon, data):\n",
    "    point = Point(lon, lat)\n",
    "    for row in data:\n",
    "        polygon = shape({\"type\": \"Polygon\", \"coordinates\": row[\"geometry\"][\"coordinates\"]})\n",
    "        if polygon.contains(point):\n",
    "            return row[\"properties\"][\"borough\"]\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "#get_borough_name_udf = udf(lambda lat, lon: get_borough_name(lat, lon, geojson_iterable), StringType())\n",
    "spark.udf.register(\"get_borough_name_udf\",lambda lat, lon: get_borough_name(lat, lon, geojson_iterable),StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc8699-6cd6-464a-867c-884d1ed8f00b",
   "metadata": {},
   "source": [
    "### 1.2.5 Limit data and create the `pickup_borough` and `dropoff_borough` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc43c432-a412-4d5b-85a1-dcffc2bad69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_bucketed = spark.table(raw_table_name)\n",
    "# Remove unwanted records.\n",
    "# 1.Rides that are longer than 4 hours (14400 seconds)\n",
    "# 2.pickup_latitude, pickup_longitude, dropoff_latitude, and dropoff_longitude that are null\n",
    "# 3.any other null columns\n",
    "limited_trip_data_bucketed=trip_data_bucketed \\\n",
    "    .where(\"trip_time_in_secs <= 14400 and pickup_latitude is not null and pickup_longitude is not null and dropoff_latitude is not null and dropoff_longitude is not null\")\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e0e7a37-3db7-47a0-8cb4-69a80e714402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select only needed columns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create the enrichment table\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#.withColumn(\"pickup_borough\", get_borough_name_udf(trip_data_bucketed.pickup_latitude, trip_data_bucketed.pickup_longitude))\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#.withColumn(\"dropoff_borough\", get_borough_name_udf(trip_data_bucketed.dropoff_latitude, trip_data_bucketed.dropoff_longitude))\\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39menrich_table_name)\n\u001b[1;32m      7\u001b[0m enriched_df \u001b[38;5;241m=\u001b[39m \u001b[43mlimited_trip_data_bucketed\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedallion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropoff_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_borough_name_udf(pickup_latitude, pickup_longitude) as pickup_borough\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m as dropoff_borough\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_partitioner\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43menrich_table_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m limited_trip_data_bucketed\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select only needed columns\n",
    "# Create the enrichment table\n",
    "#.withColumn(\"pickup_borough\", get_borough_name_udf(trip_data_bucketed.pickup_latitude, trip_data_bucketed.pickup_longitude))\\\n",
    "#.withColumn(\"dropoff_borough\", get_borough_name_udf(trip_data_bucketed.dropoff_latitude, trip_data_bucketed.dropoff_longitude))\\\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+enrich_table_name)\n",
    "enriched_df = limited_trip_data_bucketed \\\n",
    "    .selectExpr(\"medallion\",\"pickup_date\",\"pickup_datetime\",\"dropoff_datetime\"\\\n",
    "    ,\"get_borough_name_udf(pickup_latitude, pickup_longitude) as pickup_borough\"\n",
    "    ,\"'b' as dropoff_borough\")\\\n",
    "    .write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .partitionBy(\"pickup_date\")\\\n",
    "    .sortBy(\"pickup_datetime\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(enrich_table_name)\n",
    "limited_trip_data_bucketed.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43d82cc6-e1da-4ba2-bfe4-cb27885768f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in table: {} 173173881\n",
      "+--------------------+-------------------+-------------------+--------------+---------------+-----------+\n",
      "|           medallion|    pickup_datetime|   dropoff_datetime|pickup_borough|dropoff_borough|pickup_date|\n",
      "+--------------------+-------------------+-------------------+--------------+---------------+-----------+\n",
      "|D10D443F40D9B6102...|2013-02-23 00:00:00|2013-02-23 00:08:00|             a|              b| 2013-02-23|\n",
      "|D2DF0BEB0863E635B...|2013-02-23 00:00:12|2013-02-23 00:09:27|             a|              b| 2013-02-23|\n",
      "|CBA5A347BB1107C22...|2013-02-23 00:00:15|2013-02-23 00:10:56|             a|              b| 2013-02-23|\n",
      "|6EB4A57D96979FD61...|2013-02-23 00:00:27|2013-02-23 00:09:01|             a|              b| 2013-02-23|\n",
      "|4821989F782D54603...|2013-02-23 00:01:00|2013-02-23 00:26:00|             a|              b| 2013-02-23|\n",
      "|6B7BD58F0CFF19BF4...|2013-02-23 00:01:09|2013-02-23 00:01:18|             a|              b| 2013-02-23|\n",
      "|A56F3CF07F1085B21...|2013-02-23 00:02:00|2013-02-23 00:08:00|             a|              b| 2013-02-23|\n",
      "|F5A8911BD233EBAD2...|2013-02-23 00:03:00|2013-02-23 00:19:00|             a|              b| 2013-02-23|\n",
      "|0F936210DB9821900...|2013-02-23 00:03:08|2013-02-23 00:12:06|             a|              b| 2013-02-23|\n",
      "|6B7BD58F0CFF19BF4...|2013-02-23 00:04:45|2013-02-23 00:12:10|             a|              b| 2013-02-23|\n",
      "|7224604EF2AE1672E...|2013-02-23 00:05:00|2013-02-23 00:26:00|             a|              b| 2013-02-23|\n",
      "|EA3AC177801D7CD0E...|2013-02-23 00:05:00|2013-02-23 00:24:00|             a|              b| 2013-02-23|\n",
      "|5C94216F39789033B...|2013-02-23 00:06:00|2013-02-23 00:18:00|             a|              b| 2013-02-23|\n",
      "|024F5B8C15D735F62...|2013-02-23 00:06:00|2013-02-23 00:15:00|             a|              b| 2013-02-23|\n",
      "|86FD2A674F956497E...|2013-02-23 00:07:00|2013-02-23 00:26:00|             a|              b| 2013-02-23|\n",
      "|A87E81DD7B6644CF1...|2013-02-23 00:07:08|2013-02-23 00:11:10|             a|              b| 2013-02-23|\n",
      "|11C915EBB29A62EB0...|2013-02-23 00:08:00|2013-02-23 00:19:00|             a|              b| 2013-02-23|\n",
      "|FA3C5EF23DA015351...|2013-02-23 00:08:12|2013-02-23 00:17:45|             a|              b| 2013-02-23|\n",
      "|A56F3CF07F1085B21...|2013-02-23 00:09:00|2013-02-23 00:23:00|             a|              b| 2013-02-23|\n",
      "|D10D443F40D9B6102...|2013-02-23 00:09:00|2013-02-23 00:15:00|             a|              b| 2013-02-23|\n",
      "+--------------------+-------------------+-------------------+--------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_data = spark.table(enrich_table_name)\n",
    "print(\"Number of records in table: \",enriched_data.count())\n",
    "enriched_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e68dc29a-6852-45a0-976f-dd2e6faa294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173179759"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trip_data_bucketed = spark.table(raw_table_name)\n",
    "#trip_data_bucketed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "891c8f45-7212-4e9c-8572-56b211633bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   borough_code   borough_name  \\\n",
      "0             5  Staten Island   \n",
      "1             5  Staten Island   \n",
      "2             5  Staten Island   \n",
      "3             5  Staten Island   \n",
      "4             4         Queens   \n",
      "\n",
      "                                          borough_id  \\\n",
      "0  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "1  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "2  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "3  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "4  http://nyc.pediacities.com/Resource/Borough/Qu...   \n",
      "\n",
      "                                            geometry  id  \n",
      "0  POLYGON ((-74.05051 40.56642, -74.04998 40.566...   0  \n",
      "1  POLYGON ((-74.05314 40.57770, -74.05406 40.577...   1  \n",
      "2  POLYGON ((-74.15946 40.64145, -74.15998 40.641...   2  \n",
      "3  POLYGON ((-74.08221 40.64828, -74.08142 40.648...   3  \n",
      "4  POLYGON ((-73.83668 40.59495, -73.83671 40.594...   4  \n",
      "borough_code    104\n",
      "borough_name    104\n",
      "borough_id      104\n",
      "geometry        104\n",
      "id              104\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the GeoJSON file\n",
    "geojson_path = \"data/in/nyc-boroughs.geojson\"\n",
    "\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Ensure the GeoDataFrame has specific columns\n",
    "expected_columns = ['boroughCode', 'borough', '@id', 'geometry']\n",
    "for col in expected_columns:\n",
    "    if col not in gdf.columns:\n",
    "        gdf[col] = pd.NA\n",
    "\n",
    "# Rename or adjust columns if necessary\n",
    "gdf = gdf.rename(columns={\n",
    "    'boroughCode': 'borough_code',\n",
    "    'borough': 'borough_name',\n",
    "    '@id': 'borough_id'\n",
    "})\n",
    "\n",
    "# Ensure the ID is set correctly\n",
    "gdf['id'] = gdf.index\n",
    "\n",
    "# Display the processed GeoDataFrame\n",
    "print(gdf.head())\n",
    "print(gdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a35e5b5b-bdd2-435f-a371-6c9cc10fad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "def create_sjoin_udf(gdf_with_poly,join_column_name):\n",
    "    def sjoin_settlement(x, y):\n",
    "        gdf_temp = gpd.GeoDataFrame(data = [[x] for x in range(len(x))],geometry=gpd.points_from_xy(x,y),columns=['id'])\n",
    "        settlement = gpd.sjoin(gdf_temp,gdf_with_poly,how='left',op='within')#.fillna(np.nan)\n",
    "        return settlement.groupby('id').agg({'poly_ID':lambda x: list(x)}).reset_index().sort_values(by='id').loc[:,join_column_name].astype('str')# if pd.isnull(sum(x)) == False else np.nan}).reset_index().sort_values(by='id').loc[:,join_column_name]\n",
    "    return pandas_udf(sjoin_settlement, returnType=StringType())\n",
    "    \n",
    "sjoin_udf = create_sjoin_udf(gdf,'poly_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51214f55-7ade-4219-a3fa-db6957d7a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "n=1000\n",
    "\n",
    "raw_data = spark.table(raw_table_name)\n",
    "limit_raw_data=raw_data.orderBy(rand()).limit(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "807bf59e-dee9-4b6c-a1ba-b16fd45a6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data=raw_data.withColumn('poly_ID',sjoin_udf(raw_data.pickup_longitude,raw_data.pickup_latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5846a20-d5a6-4868-b810-5f459fca93a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_126/3831086110.py\", line 7, in sjoin_settlement\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\", line 8252, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 931, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py\", line 985, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'id'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgeo_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_126/3831086110.py\", line 7, in sjoin_settlement\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\", line 8252, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 931, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py\", line 985, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'id'\n"
     ]
    }
   ],
   "source": [
    "geo_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf204b8-37b0-4d12-bb33-7d8f6a27bc18",
   "metadata": {},
   "source": [
    "# 02 - Run the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc723e-2a80-44aa-bf26-039de5512d85",
   "metadata": {},
   "source": [
    "## 02.1 - Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a7876-ed5a-45fd-ab8a-6b3b59d77018",
   "metadata": {},
   "source": [
    "### 02.1.a Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "779723e5-b3eb-47d7-96e6-826ff9b0398d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.idle_time_ms(start_ms, prev_end_ms)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType\n",
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "spark.udf.register(\"idle_time_ms_udf\",idle_time_ms, LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e87bd-cb2b-488f-a0c6-0b0e73dce312",
   "metadata": {},
   "source": [
    "### 02.1.b - Build Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66905428-4deb-400e-8be3-3a701429af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col, date_format,to_date\n",
    "from datetime import datetime\n",
    "\n",
    "# require variables\n",
    "trip_data_bucketed = spark.table(raw_table_name)\n",
    "\n",
    "# Configure the window\n",
    "\n",
    "# Define the window specification with partitionBy and orderBy\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_date\"), col(\"pickup_time\"))\n",
    "# Medallion refers to the vehicle, hack_license refers to the driver\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_date\"),col(\"pickup_time\"))\n",
    "\n",
    "# TODO: Add shuffling here\n",
    "#   ...\n",
    "taxi_util_data = trip_data_bucketed\n",
    "# Extract pickup_date and pickup_time from pickup_datetime\n",
    "taxi_util_data = taxi_util_data.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\n",
    "taxi_util_data = taxi_util_data.withColumn(\"pickup_time\", date_format(col(\"pickup_datetime\"), \"HH:mm:ss\"))\n",
    "\n",
    "# Add column with pickup datetime of previous\n",
    "# NB! The default value for the lag might not be correct\n",
    "taxi_util_data_sorted = taxi_util_data.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "\n",
    "# Convert to timestamp\n",
    "taxi_util_data_sorted_ts = taxi_util_data_sorted.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000) \\\n",
    "    .withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "\n",
    "# Calculate idle time per ride\n",
    "taxi_util_data_idle = taxi_util_data_sorted_ts.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "\n",
    "# Calculate total idle time per taxi\n",
    "taxi_util_data_idle_total = taxi_util_data_idle.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaab85e-3ace-423a-b5a7-e348b894d33e",
   "metadata": {},
   "source": [
    "### 02.1.c - Display Query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acc9c9fd-09c8-421b-b371-b84e0b3d67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS \"+query1_table_name)\n",
    "taxi_util_data_idle_total.write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(query1_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c83d5b43-a74b-41a0-b42e-90491a3bd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in table:  14144\n",
      "+--------------------+-----------------+\n",
      "|           medallion|sum(idle_time_ms)|\n",
      "+--------------------+-----------------+\n",
      "|0220580F4DB64D175...|        202620000|\n",
      "|0293108C734F9B1C3...|      12092040000|\n",
      "|04C5991FD21FBCA6F...|       9339794000|\n",
      "|0925ACE65D2B99A29...|      10862520000|\n",
      "|10173154AB7597AAA...|      13198140000|\n",
      "|1855E1F1B1D92297C...|       8546340000|\n",
      "|2452625B674098557...|      12695940000|\n",
      "|24D43849571D12D4E...|       5866373000|\n",
      "|25D566E299C678396...|      14408970000|\n",
      "|281BD3C8825FF093D...|       6432812000|\n",
      "|3020BDF11FA1B155D...|      10911360000|\n",
      "|302EB02F36343A64C...|      13360262000|\n",
      "|343B4F963805B642F...|       9356340000|\n",
      "|3C4E85EE0BACAD3A7...|       3465610000|\n",
      "|55926285A4B50F996...|      11144460000|\n",
      "|576BE3CE0F330A86C...|      10355820000|\n",
      "|584FBA783588D4533...|       5736660000|\n",
      "|6C7CB6AF6A0C533AB...|      14506573000|\n",
      "|70A5A3A78A693F834...|       6489420000|\n",
      "|78FD0655D0CBD598C...|       8836740000|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1_data = spark.table(query1_table_name)\n",
    "print(\"Number of records in table: \",query1_data.count())\n",
    "query1_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a409f-aab4-4154-9cd0-115511ddc382",
   "metadata": {},
   "source": [
    "# Query 2 - Average idle time per borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79d132-2583-43e9-8868-086876fba7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "226f96eb-0cfb-4758-92cb-f1e2c3468278",
   "metadata": {},
   "source": [
    "# Query 3 - The number of trips that started and ended within the same borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33363b7e-af4d-48cc-94e5-f24a67ba8dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a2cc58-a6f9-457c-9c72-fb9e6080c6f1",
   "metadata": {},
   "source": [
    "# Query 4 -  The number of trips that started in one borough and ended in another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4503d1c8-14a5-4c2a-a829-44249e4c38cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
