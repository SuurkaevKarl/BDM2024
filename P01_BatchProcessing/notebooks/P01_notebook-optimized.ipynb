{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e3377-3e6f-4877-bcb3-a77adf2a3dec",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30ea1c-8d95-43d1-8b78-62e7130aadf8",
   "metadata": {},
   "source": [
    "1. Unzip the 7z file. Manual task.\n",
    "2. `pip install findspark` to the docker\n",
    "3. Clear the parquet file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42739bf-f734-4126-bf1d-451767d3979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data' created\n",
      "Directory 'data/in' created\n",
      "Directory 'data/out' created\n",
      "Directory 'data/out/table' created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "dirs = [\"data\",\"data/in\",\"data/out\",\"data/out/table\"]\n",
    "\n",
    "for directory in dirs:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} already exists\")\n",
    "    else:\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7319c-aa8f-42c4-9b36-621625a1a953",
   "metadata": {},
   "source": [
    "### Run this to cleanup the out folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b15be1a9-56cb-488a-8f0e-fdd25a8e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/trip_data.parquet does not exist.\n",
      "data/out/table/raw_trip_data_table has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the path to the directory containing Parquet files\n",
    "file_name=\"trip_data\"\n",
    "raw_table_name=\"raw_trip_data_table\"\n",
    "parq_output_files=[\"data/out/\"+file_name+\".parquet\",\n",
    "                  \"data/out/table/\"+raw_table_name]\n",
    "\n",
    "# Check if the directory exists\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7737-6021-44ff-9e5f-67e3906280bb",
   "metadata": {},
   "source": [
    "# 01 - Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae344-64c0-4f39-81e9-71dcd7c191a0",
   "metadata": {},
   "source": [
    "## 1.1 Load csv files to Raw layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82495462-bbfa-4ee0-a178-5c502e5c19a1",
   "metadata": {},
   "source": [
    "Raw Layer contains the base format of data. We only add bucketing and partitioning to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8034af-ab25-4d09-8510-b79a84619475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read GeoJSON with PySpark\") \\\n",
    "    .config(\"spark.executor.memory\",\"6g\") \\\n",
    "    .config(\"spark.driver.memory\",\"12g\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"2\")\\\n",
    "    .config(\"spark.memory.fraction\", \".8\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414927a3-c2c3-4fc6-a7aa-bc55b77a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d9c702-19c9-49c1-b3d2-5bac3da5e4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_name=\"trip_data\"\n",
    "file_prefix=\"data/in/\"+file_name+\"_\"\n",
    "# List of CSV files\n",
    "#csv_file_paths = [file_prefix +\"test1.csv\",file_prefix +\"test2.csv\"]\n",
    "csv_file_paths = [file_prefix + \"1.csv\" , file_prefix + \"2.csv\", file_prefix + \"3.csv\",\n",
    "                  file_prefix + \"4.csv\", file_prefix + \"5.csv\", file_prefix + \"6.csv\",\n",
    "                  file_prefix + \"7.csv\", file_prefix + \"8.csv\", file_prefix + \"9.csv\",\n",
    "                  file_prefix + \"10.csv\", file_prefix + \"11.csv\", file_prefix + \"12.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b453d9-8ce6-47c9-a081-1edce72c25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "column_partitioner=\"medallion\"\n",
    "# Read the CSV files into a single DataFrame\n",
    "raw_file_schema= StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6000182-32d6-4941-bfb1-952ad1ab184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_file_paths, header=True, schema=raw_file_schema).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf3f0b1-a183-4b43-afa6-e313050fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parquet file successful.\n"
     ]
    }
   ],
   "source": [
    "bucket_count=400\n",
    "\n",
    "# To reset the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+raw_table_name)\n",
    "df.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\\\n",
    "    .write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .partitionBy(\"pickup_date\")\\\n",
    "    .sortBy(\"pickup_datetime\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(raw_table_name)\n",
    "df.unpersist()    \n",
    "print(\"Writing parquet file successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49345af0-51dd-4782-a3cb-949851ee17c6",
   "metadata": {},
   "source": [
    "## 1.2 Display bucketed and partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29cad1f-f76f-4480-960c-9e9dd7f272fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|default  |raw_trip_data_table|false      |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+----------------------+\n",
      "|partition             |\n",
      "+----------------------+\n",
      "|pickup_date=2013-01-01|\n",
      "|pickup_date=2013-01-02|\n",
      "|pickup_date=2013-01-03|\n",
      "|pickup_date=2013-01-04|\n",
      "|pickup_date=2013-01-05|\n",
      "|pickup_date=2013-01-06|\n",
      "|pickup_date=2013-01-07|\n",
      "|pickup_date=2013-01-08|\n",
      "|pickup_date=2013-01-09|\n",
      "|pickup_date=2013-01-10|\n",
      "|pickup_date=2013-01-11|\n",
      "|pickup_date=2013-01-12|\n",
      "|pickup_date=2013-01-13|\n",
      "|pickup_date=2013-01-14|\n",
      "|pickup_date=2013-01-15|\n",
      "|pickup_date=2013-01-16|\n",
      "|pickup_date=2013-01-17|\n",
      "|pickup_date=2013-01-18|\n",
      "|pickup_date=2013-01-19|\n",
      "|pickup_date=2013-01-20|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data_bucketed = spark.table(raw_table_name)\n",
    "#trip_data_bucketed.show()\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "spark.sql(\"SHOW PARTITIONS \"+raw_table_name).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d31b0f34-472b-4b96-83d7-ac072a8ad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType\n",
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66905428-4deb-400e-8be3-3a701429af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure the window\n",
    "# Medallion refers to the vehicle, hack_license refers to the driver\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_datetime\"))\n",
    "\n",
    "# TODO: Add shuffling here\n",
    "#   ...\n",
    "taxi_util_data = trip_data_bucketed\n",
    "\n",
    "# Add column with pickup datetime of previous\n",
    "# NB! The default value for the lag might not be correct\n",
    "taxi_util_data_sorted = taxi_util_data.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "\n",
    "# Convert to timestamp\n",
    "taxi_util_data_sorted_ts = taxi_util_data_sorted.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000) \\\n",
    "    .withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "\n",
    "# Calculate idle time per ride\n",
    "taxi_util_data_idle = taxi_util_data_sorted_ts.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "\n",
    "# Calculate total idle time per taxi\n",
    "taxi_util_data_idle_total = taxi_util_data_idle.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc9c9fd-09c8-421b-b371-b84e0b3d67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           medallion|sum(idle_time_ms)|\n",
      "+--------------------+-----------------+\n",
      "|89D227B655E5C82AE...|                0|\n",
      "|0BD7C8F5BA12B88E0...|                0|\n",
      "|DFD2202EE08F7A8DC...|                0|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_util_data_idle_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608348-c0b0-40e5-972c-94e6e55d1d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
