{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e3377-3e6f-4877-bcb3-a77adf2a3dec",
   "metadata": {},
   "source": [
    "# 00 - Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30ea1c-8d95-43d1-8b78-62e7130aadf8",
   "metadata": {},
   "source": [
    "1. Unzip the 7z file. Manual task.\n",
    "2. Clear the parquet file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42739bf-f734-4126-bf1d-451767d3979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already exists\n",
      "data/in already exists\n",
      "data/out already exists\n",
      "data/out/table already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "dirs = [\"data\",\"data/in\",\"data/out\",\"data/out/table\"]\n",
    "\n",
    "for directory in dirs:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} already exists\")\n",
    "    else:\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7319c-aa8f-42c4-9b36-621625a1a953",
   "metadata": {},
   "source": [
    "### Run this to cleanup/reset the out folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15be1a9-56cb-488a-8f0e-fdd25a8e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/trip_data.parquet does not exist.\n",
      "data/out/table/raw_trip_data_table does not exist.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the path to the directory containing Parquet files\n",
    "file_name=\"trip_data\"\n",
    "raw_table_name=\"raw_trip_data_table\"\n",
    "parq_output_files=[\"data/out/\"+file_name+\".parquet\",\n",
    "                  \"data/out/table/\"+raw_table_name]\n",
    "\n",
    "# Check if the directory exists\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7737-6021-44ff-9e5f-67e3906280bb",
   "metadata": {},
   "source": [
    "# 01 - Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae344-64c0-4f39-81e9-71dcd7c191a0",
   "metadata": {},
   "source": [
    "## 1.1 Load csv files to hive table. Let's call it Raw layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82495462-bbfa-4ee0-a178-5c502e5c19a1",
   "metadata": {},
   "source": [
    "Raw Layer contains the base format of data. We only add bucketing and partitioning to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478f6c-5ac6-4676-8325-0ade01b91fce",
   "metadata": {},
   "source": [
    "### 1.1.1 Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8034af-ab25-4d09-8510-b79a84619475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM2024 - Project01\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414927a3-c2c3-4fc6-a7aa-bc55b77a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable below if you are resetting the docker image too.\n",
    "# This prouces a gazillion logs and you will scratch your head why are you losing so much disk space\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151654f-df66-41cc-ac2c-6f3f1b771603",
   "metadata": {},
   "source": [
    "### 1.1.2. List the files to be ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d9c702-19c9-49c1-b3d2-5bac3da5e4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"trip_data\"\n",
    "file_prefix=\"data/in/\"+file_name+\"_\"\n",
    "# List of CSV files\n",
    "#csv_file_paths = [file_prefix +\"test1.csv\",file_prefix +\"test2.csv\"]\n",
    "csv_file_paths = [file_prefix + \"1.csv\" , file_prefix + \"2.csv\", file_prefix + \"3.csv\",\n",
    "                  file_prefix + \"4.csv\", file_prefix + \"5.csv\", file_prefix + \"6.csv\",\n",
    "                  file_prefix + \"7.csv\", file_prefix + \"8.csv\", file_prefix + \"9.csv\",\n",
    "                  file_prefix + \"10.csv\", file_prefix + \"11.csv\", file_prefix + \"12.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d31c5-d0ab-44ee-919b-dd3796088df3",
   "metadata": {},
   "source": [
    "### 1.1.3. Declare the schema to save some resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b453d9-8ce6-47c9-a081-1edce72c25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "column_partitioner=\"medallion\"\n",
    "\n",
    "raw_file_schema= StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfae9b7-df07-4825-8d58-778d02aa40e5",
   "metadata": {},
   "source": [
    "### 1.1.4 Read the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3a2dc-f685-459f-bab3-b01e5832349f",
   "metadata": {},
   "source": [
    "Add the persists so that the file remains to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6000182-32d6-4941-bfb1-952ad1ab184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_file_paths, header=True, schema=raw_file_schema).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d283f3b-2ebd-42cc-894f-f21b5f40762c",
   "metadata": {},
   "source": [
    "### 1.1.5. Create the hive table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed86530-8ceb-4f7e-b1b0-c53ddbdcad50",
   "metadata": {},
   "source": [
    "Apply `bucketing` and `partitioning` to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf3f0b1-a183-4b43-afa6-e313050fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parquet file successful.\n"
     ]
    }
   ],
   "source": [
    "bucket_count=400\n",
    "\n",
    "# To reset the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+raw_table_name)\n",
    "df.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\\\n",
    "    .write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .partitionBy(\"pickup_date\")\\\n",
    "    .sortBy(\"pickup_datetime\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(raw_table_name)\n",
    "df.unpersist()    \n",
    "print(\"Writing parquet file successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49345af0-51dd-4782-a3cb-949851ee17c6",
   "metadata": {},
   "source": [
    "### 1.1.6 Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67652ff8-d040-449e-b9c2-e24b26b3197d",
   "metadata": {},
   "source": [
    "Check the details from the Raw Layer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29cad1f-f76f-4480-960c-9e9dd7f272fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|default  |raw_trip_data_table|false      |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+----------------------+\n",
      "|partition             |\n",
      "+----------------------+\n",
      "|pickup_date=2013-01-01|\n",
      "|pickup_date=2013-01-02|\n",
      "|pickup_date=2013-01-03|\n",
      "|pickup_date=2013-01-04|\n",
      "|pickup_date=2013-01-05|\n",
      "|pickup_date=2013-01-06|\n",
      "|pickup_date=2013-01-07|\n",
      "|pickup_date=2013-01-08|\n",
      "|pickup_date=2013-01-09|\n",
      "|pickup_date=2013-01-10|\n",
      "|pickup_date=2013-01-11|\n",
      "|pickup_date=2013-01-12|\n",
      "|pickup_date=2013-01-13|\n",
      "|pickup_date=2013-01-14|\n",
      "|pickup_date=2013-01-15|\n",
      "|pickup_date=2013-01-16|\n",
      "|pickup_date=2013-01-17|\n",
      "|pickup_date=2013-01-18|\n",
      "|pickup_date=2013-01-19|\n",
      "|pickup_date=2013-01-20|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data_bucketed = spark.table(raw_table_name)\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "spark.sql(\"SHOW PARTITIONS \"+raw_table_name).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e8bbe-01f9-45f8-b460-79f6c44c8b7d",
   "metadata": {},
   "source": [
    "## 1.2 Enrich the data with geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406674c-3a9c-4da5-9aa0-53e1e9f1ee3a",
   "metadata": {},
   "source": [
    "### 1.2.1. Read geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0e2789-84bb-4ae5-9034-57a3a8ea0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, LongType, ArrayType\n",
    "\n",
    "# https://gist.github.com/JulesBelveze/a552e8c53dfd1f46948cbeb32c096611\n",
    "geojson_data=\"data/in/nyc-boroughs.geojson\"\n",
    "# Define the custom schema\n",
    "geojson_schema = StructType([\n",
    "    StructField(\"type\", StringType(), nullable=True),\n",
    "    StructField(\"id\", LongType(), nullable=True),\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"boroughCode\", LongType(), nullable=True),\n",
    "        StructField(\"borough\", StringType(), nullable=True),\n",
    "        StructField(\"@id\", StringType(), nullable=True)\n",
    "    ]), nullable=True),\n",
    "    StructField(\"geometry\", StructType([\n",
    "        StructField(\"type\", StringType(), nullable=True),\n",
    "        StructField(\"coordinates\", ArrayType(ArrayType(ArrayType(DoubleType()))), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Read GeoJSON file with custom schema\n",
    "geojson_df = spark.read.schema(geojson_schema).json(geojson_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c73af24-a7b2-4eb7-bde6-7044ffac468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "+-------+----+--------------------+--------------------+\n",
      "|   type|  id|          properties|            geometry|\n",
      "+-------+----+--------------------+--------------------+\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|   NULL|NULL|                NULL|                NULL|\n",
      "|Feature|   8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+---+--------------------+--------------------+\n",
      "|   type| id|          properties|            geometry|\n",
      "+-------+---+--------------------+--------------------+\n",
      "|Feature|  0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  9|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 10|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 11|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 12|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 13|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 14|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 15|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 16|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 17|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 18|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 19|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of boroughs: 104\n"
     ]
    }
   ],
   "source": [
    "# View the schema\n",
    "geojson_df.printSchema()\n",
    "\n",
    "# Show the data\n",
    "geojson_df.show()\n",
    "geojson_df = geojson_df.na.drop() #remove the NULL values\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "geojson_df.show()\n",
    "\n",
    "num_rows = geojson_df.count()\n",
    "print(f\"Number of boroughs: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ae8c66-92ce-4154-bd10-136dce9c9486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.11/site-packages (from shapely) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31b0f34-472b-4b96-83d7-ac072a8ad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType\n",
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66905428-4deb-400e-8be3-3a701429af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure the window\n",
    "\n",
    "# Define the window specification with partitionBy and orderBy\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_date\"), col(\"pickup_time\"))\n",
    "# Medallion refers to the vehicle, hack_license refers to the driver\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_date\"),col(\"pickup_time\"))\n",
    "\n",
    "# TODO: Add shuffling here\n",
    "#   ...\n",
    "taxi_util_data = trip_data_bucketed\n",
    "# Extract pickup_date and pickup_time from pickup_datetime\n",
    "taxi_util_data = taxi_util_data.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\n",
    "taxi_util_data = taxi_util_data.withColumn(\"pickup_time\", date_format(col(\"pickup_datetime\"), \"HH:mm:ss\"))\n",
    "\n",
    "# Add column with pickup datetime of previous\n",
    "# NB! The default value for the lag might not be correct\n",
    "taxi_util_data_sorted = taxi_util_data.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "\n",
    "# Convert to timestamp\n",
    "taxi_util_data_sorted_ts = taxi_util_data_sorted.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000) \\\n",
    "    .withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "\n",
    "# Calculate idle time per ride\n",
    "taxi_util_data_idle = taxi_util_data_sorted_ts.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "\n",
    "# Calculate total idle time per taxi\n",
    "taxi_util_data_idle_total = taxi_util_data_idle.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc9c9fd-09c8-421b-b371-b84e0b3d67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           medallion|sum(idle_time_ms)|\n",
      "+--------------------+-----------------+\n",
      "|1109955CCAABCBCE1...|       7789070000|\n",
      "|18DDC049CC26AE3AA...|       2965980000|\n",
      "|1D8B19995A8EBDF9B...|       5282700000|\n",
      "|223670562219093D6...|      10122420000|\n",
      "|2E7374B2849EB39CD...|       5122665000|\n",
      "|35B2F21FAF5E53F1E...|      13082705000|\n",
      "|57E8E649531AB8807...|      13253708000|\n",
      "|586D9BD604B923DA3...|      12173972000|\n",
      "|595917A7813CC80DA...|      14438432000|\n",
      "|59DF6039EC312EE6D...|      11192880000|\n",
      "|6695FB6E06F7D99F5...|      12821718000|\n",
      "|72EAFBA3FB9F0507C...|      12918600000|\n",
      "|73039762E0F4B253E...|      13207440000|\n",
      "|73F27D8C8C3B37D04...|       7252834000|\n",
      "|753BC0484097BB236...|      13198400000|\n",
      "|764CA5AE502C0FEC9...|      12894830000|\n",
      "|846DFE2D59F6E76EC...|       8048640000|\n",
      "|87EB479F55B88D47C...|      10627020000|\n",
      "|963BEE5F306952D20...|      12738530000|\n",
      "|98C89210E1228F7AE...|       8587942000|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_util_data_idle_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608348-c0b0-40e5-972c-94e6e55d1d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
