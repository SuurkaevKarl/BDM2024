{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e3377-3e6f-4877-bcb3-a77adf2a3dec",
   "metadata": {},
   "source": [
    "# 00 - Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30ea1c-8d95-43d1-8b78-62e7130aadf8",
   "metadata": {},
   "source": [
    "1. Unzip the 7z file. Manual task.\n",
    "2. Clear the parquet file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42739bf-f734-4126-bf1d-451767d3979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data' created\n",
      "Directory 'data/in' created\n",
      "Directory 'data/out' created\n",
      "Directory 'data/out/table' created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "dirs = [\"data\",\"data/in\",\"data/out\",\"data/out/table\"]\n",
    "\n",
    "for directory in dirs:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} already exists\")\n",
    "    else:\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7319c-aa8f-42c4-9b36-621625a1a953",
   "metadata": {},
   "source": [
    "### Run this to cleanup/reset the out folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15be1a9-56cb-488a-8f0e-fdd25a8e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/trip_data.parquet does not exist.\n",
      "data/out/table/raw_trip_data_table does not exist.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the path to the directory containing Parquet files\n",
    "file_name=\"trip_data\"\n",
    "raw_table_name=\"raw_trip_data_table\"\n",
    "parq_output_files=[\"data/out/\"+file_name+\".parquet\",\n",
    "                  \"data/out/table/\"+raw_table_name]\n",
    "\n",
    "# Check if the directory exists\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7737-6021-44ff-9e5f-67e3906280bb",
   "metadata": {},
   "source": [
    "# 01 - Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae344-64c0-4f39-81e9-71dcd7c191a0",
   "metadata": {},
   "source": [
    "## 1.1 Load csv files to hive table. Let's call it Raw layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82495462-bbfa-4ee0-a178-5c502e5c19a1",
   "metadata": {},
   "source": [
    "Raw Layer contains the base format of data. We only add bucketing and partitioning to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478f6c-5ac6-4676-8325-0ade01b91fce",
   "metadata": {},
   "source": [
    "### 1.1.1 Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8034af-ab25-4d09-8510-b79a84619475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM2024 - Project01\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414927a3-c2c3-4fc6-a7aa-bc55b77a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable below if you are resetting the docker image too.\n",
    "# This prouces a gazillion logs and you will scratch your head why are you losing so much disk space\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151654f-df66-41cc-ac2c-6f3f1b771603",
   "metadata": {},
   "source": [
    "### 1.1.2. List the files to be ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d9c702-19c9-49c1-b3d2-5bac3da5e4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"trip_data\"\n",
    "file_prefix=\"data/in/\"+file_name+\"_\"\n",
    "# List of CSV files\n",
    "#csv_file_paths = [file_prefix +\"test1.csv\",file_prefix +\"test2.csv\"]\n",
    "csv_file_paths = [file_prefix + \"1.csv\" , file_prefix + \"2.csv\", file_prefix + \"3.csv\",\n",
    "                  file_prefix + \"4.csv\", file_prefix + \"5.csv\", file_prefix + \"6.csv\",\n",
    "                  file_prefix + \"7.csv\", file_prefix + \"8.csv\", file_prefix + \"9.csv\",\n",
    "                  file_prefix + \"10.csv\", file_prefix + \"11.csv\", file_prefix + \"12.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d31c5-d0ab-44ee-919b-dd3796088df3",
   "metadata": {},
   "source": [
    "### 1.1.3. Declare the schema to save some resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b453d9-8ce6-47c9-a081-1edce72c25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "column_partitioner=\"medallion\"\n",
    "\n",
    "raw_file_schema= StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfae9b7-df07-4825-8d58-778d02aa40e5",
   "metadata": {},
   "source": [
    "### 1.1.4 Read the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3a2dc-f685-459f-bab3-b01e5832349f",
   "metadata": {},
   "source": [
    "Add the persists so that the file remains to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6000182-32d6-4941-bfb1-952ad1ab184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_file_paths, header=True, schema=raw_file_schema).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d283f3b-2ebd-42cc-894f-f21b5f40762c",
   "metadata": {},
   "source": [
    "### 1.1.5. Create the hive table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed86530-8ceb-4f7e-b1b0-c53ddbdcad50",
   "metadata": {},
   "source": [
    "Apply `bucketing` and `partitioning` to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cf3f0b1-a183-4b43-afa6-e313050fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.util.Utils does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# To reset the table\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mraw_table_name)\n\u001b[1;32m      5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_partitioner\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_table_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39munpersist()    \n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting parquet file successful.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:157\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkUpgradeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[0;32m--> 157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUtils\u001b[49m\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     is_instance_of(gw, c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.api.python.PythonException\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1664\u001b[0m, in \u001b[0;36mJavaPackage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaClass(\n\u001b[1;32m   1662\u001b[0m         answer[proto\u001b[38;5;241m.\u001b[39mCLASS_FQN_START:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(new_fqn))\n",
      "\u001b[0;31mPy4JError\u001b[0m: org.apache.spark.util.Utils does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "bucket_count=400\n",
    "\n",
    "# To reset the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+raw_table_name)\n",
    "df.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\\\n",
    "    .write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .partitionBy(\"pickup_date\")\\\n",
    "    .sortBy(\"pickup_datetime\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(raw_table_name)\n",
    "df.unpersist()    \n",
    "print(\"Writing parquet file successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49345af0-51dd-4782-a3cb-949851ee17c6",
   "metadata": {},
   "source": [
    "### 1.1.6 Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67652ff8-d040-449e-b9c2-e24b26b3197d",
   "metadata": {},
   "source": [
    "Check the details from the Raw Layer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29cad1f-f76f-4480-960c-9e9dd7f272fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|default  |raw_trip_data_table|false      |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+----------------------+\n",
      "|partition             |\n",
      "+----------------------+\n",
      "|pickup_date=2013-01-01|\n",
      "|pickup_date=2013-01-02|\n",
      "|pickup_date=2013-01-03|\n",
      "|pickup_date=2013-01-04|\n",
      "|pickup_date=2013-01-05|\n",
      "|pickup_date=2013-01-06|\n",
      "|pickup_date=2013-01-07|\n",
      "|pickup_date=2013-01-08|\n",
      "|pickup_date=2013-01-09|\n",
      "|pickup_date=2013-01-10|\n",
      "|pickup_date=2013-01-11|\n",
      "|pickup_date=2013-01-12|\n",
      "|pickup_date=2013-01-13|\n",
      "|pickup_date=2013-01-14|\n",
      "|pickup_date=2013-01-15|\n",
      "|pickup_date=2013-01-16|\n",
      "|pickup_date=2013-01-17|\n",
      "|pickup_date=2013-01-18|\n",
      "|pickup_date=2013-01-19|\n",
      "|pickup_date=2013-01-20|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data_bucketed = spark.table(raw_table_name)\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "spark.sql(\"SHOW PARTITIONS \"+raw_table_name).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e8bbe-01f9-45f8-b460-79f6c44c8b7d",
   "metadata": {},
   "source": [
    "## 1.2 Enrich the data with geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406674c-3a9c-4da5-9aa0-53e1e9f1ee3a",
   "metadata": {},
   "source": [
    "### 1.2.1. Read geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0e2789-84bb-4ae5-9034-57a3a8ea0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, LongType, ArrayType\n",
    "\n",
    "# https://gist.github.com/JulesBelveze/a552e8c53dfd1f46948cbeb32c096611\n",
    "geojson_data=\"data/in/nyc-boroughs.geojson\"\n",
    "# Define the custom schema\n",
    "geojson_schema = StructType([\n",
    "    StructField(\"type\", StringType(), nullable=True),\n",
    "    StructField(\"id\", LongType(), nullable=True),\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"boroughCode\", LongType(), nullable=True),\n",
    "        StructField(\"borough\", StringType(), nullable=True),\n",
    "        StructField(\"@id\", StringType(), nullable=True)\n",
    "    ]), nullable=True),\n",
    "    StructField(\"geometry\", StructType([\n",
    "        StructField(\"type\", StringType(), nullable=True),\n",
    "        StructField(\"coordinates\", ArrayType(ArrayType(ArrayType(DoubleType()))), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Read GeoJSON file with custom schema\n",
    "geojson_df = spark.read.schema(geojson_schema).json(geojson_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c73af24-a7b2-4eb7-bde6-7044ffac468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "+-------+---+--------------------+--------------------+\n",
      "|   type| id|          properties|            geometry|\n",
      "+-------+---+--------------------+--------------------+\n",
      "|Feature|  0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  9|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 10|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 11|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 12|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 13|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 14|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 15|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 16|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 17|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 18|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 19|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+---+--------------------+--------------------+\n",
      "|   type| id|          properties|            geometry|\n",
      "+-------+---+--------------------+--------------------+\n",
      "|Feature|  0|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  1|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  2|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  3|{5, Staten Island...|{Polygon, [[[-74....|\n",
      "|Feature|  4|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  5|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  6|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  7|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  8|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature|  9|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 10|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 11|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 12|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 13|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 14|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 15|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 16|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 17|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 18|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "|Feature| 19|{4, Queens, http:...|{Polygon, [[[-73....|\n",
      "+-------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of boroughs: 104\n"
     ]
    }
   ],
   "source": [
    "# View the schema\n",
    "geojson_df.printSchema()\n",
    "\n",
    "# Show the data\n",
    "geojson_df.show()\n",
    "geojson_df = geojson_df.na.drop() #remove the NULL values\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "geojson_df.show()\n",
    "\n",
    "num_rows = geojson_df.count()\n",
    "print(f\"Number of boroughs: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41123961-2fce-47c9-a6f5-0ab5c132dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae8c66-92ce-4154-bd10-136dce9c9486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))': /simple/shapely/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))': /simple/shapely/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))': /simple/shapely/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))': /simple/shapely/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))': /simple/shapely/\u001b[0m\u001b[33m\n",
      "\u001b[0mCould not fetch URL https://pypi.org/simple/shapely/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/shapely/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))) - skipping\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement shapely (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for shapely\u001b[0m\u001b[31m\n",
      "\u001b[0mCould not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)'))) - skipping\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d31b0f34-472b-4b96-83d7-ac072a8ad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType\n",
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66905428-4deb-400e-8be3-3a701429af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure the window\n",
    "# Medallion refers to the vehicle, hack_license refers to the driver\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_datetime\"))\n",
    "\n",
    "# TODO: Add shuffling here\n",
    "#   ...\n",
    "taxi_util_data = trip_data_bucketed\n",
    "\n",
    "# Add column with pickup datetime of previous\n",
    "# NB! The default value for the lag might not be correct\n",
    "taxi_util_data_sorted = taxi_util_data.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "\n",
    "# Convert to timestamp\n",
    "taxi_util_data_sorted_ts = taxi_util_data_sorted.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000) \\\n",
    "    .withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "\n",
    "# Calculate idle time per ride\n",
    "taxi_util_data_idle = taxi_util_data_sorted_ts.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "\n",
    "# Calculate total idle time per taxi\n",
    "taxi_util_data_idle_total = taxi_util_data_idle.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc9c9fd-09c8-421b-b371-b84e0b3d67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           medallion|sum(idle_time_ms)|\n",
      "+--------------------+-----------------+\n",
      "|89D227B655E5C82AE...|                0|\n",
      "|0BD7C8F5BA12B88E0...|                0|\n",
      "|DFD2202EE08F7A8DC...|                0|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_util_data_idle_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608348-c0b0-40e5-972c-94e6e55d1d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
