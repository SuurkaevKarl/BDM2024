{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e3377-3e6f-4877-bcb3-a77adf2a3dec",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30ea1c-8d95-43d1-8b78-62e7130aadf8",
   "metadata": {},
   "source": [
    "1. Unzip the 7z file. Manual task.\n",
    "2. `pip install findspark` to the docker\n",
    "3. Clear the parquet file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15be1a9-56cb-488a-8f0e-fdd25a8e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out/trip_data.parquet does not exist.\n",
      "data/out/table/trip_data_table does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path to the directory containing Parquet files\n",
    "file_name=\"trip_data\"\n",
    "table_name=\"trip_data_table\"\n",
    "parq_output_files=[\"data/out/\"+file_name+\".parquet\",\n",
    "                  \"data/out/table/\"+table_name]\n",
    "\n",
    "# Check if the directory exists\n",
    "for file in parq_output_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.rmtree(file)\n",
    "        print(f'{file} has been deleted.')\n",
    "    else:\n",
    "        print(f'{file} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7737-6021-44ff-9e5f-67e3906280bb",
   "metadata": {},
   "source": [
    "# 01 - Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae344-64c0-4f39-81e9-71dcd7c191a0",
   "metadata": {},
   "source": [
    "## 1.1 Partition data and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8034af-ab25-4d09-8510-b79a84619475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read GeoJSON with PySpark\") \\\n",
    "    .config(\"spark.master\", \"local[6]\")\\\n",
    "    #.config(\"spark.worker.memory\",\"2g\") \\\n",
    "    #.config(\"spark.worker.cores\",\"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"6\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.driver.cores\", \"4\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data/out/table\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414927a3-c2c3-4fc6-a7aa-bc55b77a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9c702-19c9-49c1-b3d2-5bac3da5e4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_name=\"trip_data\"\n",
    "file_prefix=\"data/in/\"+file_name+\"_\"\n",
    "# List of CSV files\n",
    "csv_file_paths = [file_prefix +\"test.csv\"]\n",
    "#csv_file_paths = [file_prefix + \"1.csv\" ]#, file_prefix + \"2.csv\"]#, file_prefix + \"3.csv\",\n",
    "                  #file_prefix + \"4.csv\", file_prefix + \"5.csv\", file_prefix + \"6.csv\",\n",
    "                  #file_prefix + \"7.csv\", file_prefix + \"8.csv\", file_prefix + \"9.csv\",\n",
    "                  #file_prefix + \"10.csv\", file_prefix + \"11.csv\", file_prefix + \"12.csv\"]\n",
    "\n",
    "\n",
    "parq_output_file=\"data/out/\"+file_name+\".parquet\"\n",
    "column_partitioner=\"medallion\"\n",
    "# Read the CSV files into a single DataFrame\n",
    "df = spark.read.csv(csv_file_paths, header=True, inferSchema=True)\n",
    "#df.show()\n",
    "bucket_count=3000\n",
    "df.write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .bucketBy(bucket_count,column_partitioner)\\\n",
    "    .mode(\"append\")\\\n",
    "    .saveAsTable(table_name)\n",
    "                    \n",
    "print(\"Writing parquet file successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49345af0-51dd-4782-a3cb-949851ee17c6",
   "metadata": {},
   "source": [
    "## 1.2 Load the saved parquet format and apply bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cad1f-f76f-4480-960c-9e9dd7f272fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parq_input_file=parq_output_file\n",
    "parq_df=spark.read.parquet(parq_input_file)\n",
    "bucket_table_name=\"trip_data_bucketed\"\n",
    "# To reset the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+bucket_table_name)\n",
    "# Apply bucketing and ordering\n",
    "bucketed_df = parq_df.write.bucketBy(num_buckets, column_partitioner) \\\n",
    "                    .sortBy(\"pickup_datetime\") \\\n",
    "                    .saveAsTable(bucket_table_name)\n",
    "trip_data_bucketed = spark.table(bucket_table_name)\n",
    "trip_data_bucketed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2099b4-d782-475b-84fe-8aa03aa9852f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d31b0f34-472b-4b96-83d7-ac072a8ad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType\n",
    "# Create UDF for finding idle time between two times\n",
    "def idle_time_ms(start_ms, prev_end_ms):\n",
    "    if start_ms is None or prev_end_ms is None:\n",
    "        return 0\n",
    "    idle_ms = start_ms - prev_end_ms\n",
    "    threshold_duration_ms = 4 * 60 * 60 * 1000  # 4 hours in milliseconds\n",
    "    if idle_ms < 0 or idle_ms > threshold_duration_ms:\n",
    "        return 0\n",
    "    else:\n",
    "        return idle_ms\n",
    "    \n",
    "# Define as UDF\n",
    "idle_time_ms_udf = udf(idle_time_ms, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66905428-4deb-400e-8be3-3a701429af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, sum, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure the window\n",
    "# Medallion refers to the vehicle, hack_license refers to the driver\n",
    "window_conf = Window.partitionBy(col(\"medallion\")).orderBy(col(\"pickup_datetime\"))\n",
    "\n",
    "# TODO: Add shuffling here\n",
    "#   ...\n",
    "taxi_util_data = trip_data_bucketed\n",
    "\n",
    "# Add column with pickup datetime of previous\n",
    "# NB! The default value for the lag might not be correct\n",
    "taxi_util_data_sorted = taxi_util_data.withColumn(\"dropoff_datetime_prev\", lag(col(\"dropoff_datetime\"), default=datetime.min).over(window_conf))\n",
    "\n",
    "# Convert to timestamp\n",
    "taxi_util_data_sorted_ts = taxi_util_data_sorted.withColumn(\"pickup_ts_ms\", unix_timestamp(\"pickup_datetime\") * 1000) \\\n",
    "    .withColumn(\"dropoff_prev_ts_ms\", unix_timestamp(\"dropoff_datetime_prev\") * 1000)\n",
    "\n",
    "# Calculate idle time per ride\n",
    "taxi_util_data_idle = taxi_util_data_sorted_ts.withColumn(\"idle_time_ms\", idle_time_ms_udf(\"pickup_ts_ms\", \"dropoff_prev_ts_ms\"))\n",
    "\n",
    "# Calculate total idle time per taxi\n",
    "taxi_util_data_idle_total = taxi_util_data_idle.groupBy(col(\"medallion\")).agg(sum(col(\"idle_time_ms\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc9c9fd-09c8-421b-b371-b84e0b3d67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           medallion|sum(idle_time_ms)|\n",
      "+--------------------+-----------------+\n",
      "|89D227B655E5C82AE...|                0|\n",
      "|0BD7C8F5BA12B88E0...|                0|\n",
      "|DFD2202EE08F7A8DC...|                0|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_util_data_idle_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608348-c0b0-40e5-972c-94e6e55d1d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
