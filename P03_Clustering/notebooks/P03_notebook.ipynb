{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316839b1-2c21-413d-bff1-08c9f0f58b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext represents the connection to a Spark cluster\n",
    "from pyspark.context import SparkContext\n",
    "# Configuration for a Spark application\n",
    "from pyspark.conf import SparkConf\n",
    "# The entry point to programming Spark with the Dataset and DataFrame API\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"P03_Clustering\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    #.config(\"spark.sql.repl.eagerEval.truncate\", 500) \\\n",
    "    #.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5458e95a-802e-4cd7-ba14-0083aea1db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dblp_ref_file_path = \"dblp-ref/dblp-ref-0.json\"\n",
    "papers_df = spark.read.json(dblp_ref_file_path)\n",
    "\n",
    "papers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f9a210-2378-43f8-bb79-13bf78767c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|                NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|                NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|                NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|                NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|                NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|\n",
      "|                NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|\n",
      "|                NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|\n",
      "|                NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|                NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|                NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6254-b384-456a-9f92-365651aa575a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70043032-86b9-431b-8043-366e6ecc62be",
   "metadata": {},
   "source": [
    "### Required packages and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef225f03-a517-4e63-9f59-cea33a9b1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83015362-a6fc-444a-bc5c-5019959e649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fast-langdetect in /opt/conda/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: fasttext-wheel>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (0.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (2.31.0)\n",
      "Requirement already satisfied: robust-downloader>=0.0.2 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (0.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.11/site-packages (from fasttext-wheel>=0.9.2->fast-langdetect) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from fasttext-wheel>=0.9.2->fast-langdetect) (69.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (2024.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from robust-downloader>=0.0.2->fast-langdetect) (4.66.2)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from robust-downloader>=0.0.2->fast-langdetect) (6.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fast-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbff3da5-a400-476c-85bf-021a4da98483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If you're running the language detection for the first time, try running this if the dataframe is filtered to empty\n",
    "# On first time run it might need to download a small language file that the udf might not trigger\n",
    "from fast_langdetect import detect_langs\n",
    "\n",
    "print(detect_langs(\"Hello, world!\") == 'EN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef45372e-05af-436d-b0a5-0b4685498d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, split\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from langdetect import detect, LangDetectException\n",
    "from fast_langdetect import detect_langs\n",
    "\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', \n",
    "                     'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return regexp_replace(text, r'[!()\\[\\]{};:\"\\,<>./?@#$%^&*_~]', '')\n",
    "\n",
    "# Language detection (EN)\n",
    "def detect_language(text):\n",
    "    result = False\n",
    "    try:\n",
    "        split_text = text.split(\" \")\n",
    "        if len(split_text) < 9:\n",
    "            result = detect(text) == 'en'\n",
    "        else:\n",
    "            result = \" \".join(split_text[:9]) == 'en'\n",
    "    except LangDetectException:\n",
    "        result = False\n",
    "    return result\n",
    "\n",
    "detect_language_udf = udf(detect_language, BooleanType())\n",
    "\n",
    "# ASCII detection - unused experiment but left it here for reference\n",
    "def detect_ascii(text):\n",
    "    return text.isascii()\n",
    "\n",
    "detect_ascii_udf = udf(detect_ascii, BooleanType())\n",
    "\n",
    "# Fast language detection (EN)\n",
    "def fast_detect_language(text):\n",
    "    result = False\n",
    "    try:\n",
    "        result = detect_langs(text) == 'EN'\n",
    "    except Exception:\n",
    "        result = False\n",
    "    return result\n",
    "\n",
    "fast_detect_language_udf = udf(fast_detect_language, BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2248c-c4db-479d-b09e-7e8d50c7903c",
   "metadata": {},
   "source": [
    "### Set up the required filtering and preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06241d25-9531-464c-baf3-60ef731ca643",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_cleaned_df = papers_df.filter(col(\"abstract\").isNotNull() & (col(\"abstract\").rlike(r'\\w')))\n",
    "#papers_cleaned_df = papers_cleaned_df.limit(100) # Useful for testing, takes only the first n entries\n",
    "#papers_cleaned_df = papers_cleaned_df.filter(detect_ascii_udf(col(\"title\"))) # \"Language detection\" based on if the title contains any non-ASCII characters (slow and kind of wrong)\n",
    "#papers_cleaned_df = papers_cleaned_df.filter(detect_language_udf(col(\"title\"))) # Language detection using the base language library (very slow)\n",
    "papers_cleaned_df = papers_cleaned_df.filter(fast_detect_language_udf(col(\"title\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"abstract\", remove_punctuation(col(\"abstract\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"abstract\", lower(col(\"abstract\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"words\", split(col(\"abstract\"), \" \"))\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "papers_cleaned_df = remover.transform(papers_cleaned_df)\n",
    "\n",
    "def remove_custom_stop_words(words, custom_stop_words):\n",
    "    if words is None:\n",
    "        return None\n",
    "    return [word for word in words if word not in custom_stop_words]\n",
    "\n",
    "remove_custom_stop_words_udf = udf(lambda words: remove_custom_stop_words(words, custom_stop_words), ArrayType(StringType()))\n",
    "\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"final_filtered\", remove_custom_stop_words_udf(col(\"filtered_words\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af405016-9837-4c7b-b70f-3a1dd15b1661",
   "metadata": {},
   "source": [
    "### Run the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3f6e03-bfa6-4bdf-a450-658889929684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resulting papers: 743421\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|               words|      filtered_words|      final_filtered|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
      "|the purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|[the, purpose, of...|[purpose, study, ...|[purpose, study, ...|\n",
      "|this paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|[this, paper, des...|[paper, describes...|[paper, describes...|\n",
      "|this article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|[this, article, a...|[article, applied...|[article, applied...|\n",
      "|recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|[recent, achievem...|[recent, achievem...|[recent, achievem...|\n",
      "|recently bridges ...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|[recently, bridge...|[recently, bridge...|[recently, bridge...|\n",
      "|most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|[most, applicatio...|[applications, ab...|[applications, ab...|\n",
      "|three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|[three, speech, t...|[three, speech, t...|[three, speech, t...|\n",
      "|this paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|[this, paper, foc...|[paper, focuses, ...|[paper, focuses, ...|\n",
      "|embedded systems ...|[Matias Madou, Bj...|01047814-b615-444...|        50|                NULL|Link-time compact...|european symposiu...|2004|[embedded, system...|[embedded, system...|[embedded, system...|\n",
      "|xax is a browser ...|[John R. Douceur,...|010d4ce9-0279-416...|        50|[0abc9de7-e047-44...|Leveraging legacy...|operating systems...|2008|[xax, is, a, brow...|[xax, browser, pl...|[xax, browser, pl...|\n",
      "|in recent years m...|[Hiroshi Furukawa...|010d9907-45ef-459...|         7|[b2f0e0d3-0071-40...|A pedestrian navi...|international con...|2013|[in, recent, year...|[recent, years, m...|[recent, years, m...|\n",
      "|previous language...|[Carmen Fernández...|012b88ae-a763-45d...|        50|[00515e82-3da6-49...|Word pairs in lan...|                    |2004|[previous, langua...|[previous, langua...|[previous, langua...|\n",
      "|spatial encryptio...|[Michel Abdalla, ...|016a9a21-e882-4cd...|        50|[3140c9ba-8d98-42...|Leakage-Resilient...|                    |2012|[spatial, encrypt...|[spatial, encrypt...|[spatial, encrypt...|\n",
      "|in system operati...|[Mark Burgess, Al...|01705f09-d395-4a0...|         8|[363278e2-6c9d-45...|On system rollbac...|The Journal of Lo...|2011|[in, system, oper...|[system, operatio...|[system, operatio...|\n",
      "|business strategy...|[Constantinos Gia...|01b6f2ca-3903-419...|        12|[0ce446cf-4f4f-49...|Model-Driven Stra...|                    |2012|[business, strate...|[business, strate...|[business, strate...|\n",
      "|ftp mirror tracke...|[Alexei Novikov, ...|01ccb92f-46f1-400...|         0|                  []|FTP Mirror Tracke...|usenix large inst...|2000|[ftp, mirror, tra...|[ftp, mirror, tra...|[ftp, mirror, tra...|\n",
      "|there are a numbe...|[Theresa Beauboue...|01edeac9-cd8b-46f...|         0|[09b666de-6279-48...|Information Syste...|                    |2014|[there, are, a, n...|[number, alternat...|[number, alternat...|\n",
      "|breast cancer is ...|[Bartosz Krawczyk...|0265aea8-65f3-4f4...|        50|[16f0341b-538d-40...|Breast Cancer Ide...|                    |2013|[breast, cancer, ...|[breast, cancer, ...|[breast, cancer, ...|\n",
      "|the development o...|[J. Niblock, Jian...|028d37c8-b571-41b...|         0|[8fc06656-ed2a-46...|Automated Object ...|international con...|2008|[the, development...|[development, aut...|[development, aut...|\n",
      "|quality specified...|[Qiong Liu, You Y...|02a5e8a8-061e-4c1...|         3|[04f8d077-765a-41...|Quality Assessmen...|conference on mul...|2013|[quality, specifi...|[quality, specifi...|[quality, specifi...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "The process took 18.212321996688843 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_ms = time.time_ns() / 1_000_000_000\n",
    "print(\"Number of resulting papers:\", papers_cleaned_df.count())\n",
    "\n",
    "print()\n",
    "papers_cleaned_df.show()\n",
    "print()\n",
    "\n",
    "end_ms = time.time_ns() / 1_000_000_000\n",
    "print(\"The process took\", (end_ms - start_ms), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebe98b-8319-4175-82c4-93ad35fa2c3f",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7da1058-f2b1-4520-a488-f1ca1faa6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import when\n",
    "#NB! this takes a long time to calculate so don't run this often (run time of ~1 minute)\n",
    "hashing_tf = HashingTF(inputCol=\"final_filtered\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "tf_df = hashing_tf.transform(papers_cleaned_df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "tfidf_df = tfidf_df.filter(col(\"features\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7852e1e0-dc7b-4eb5-8bc4-abcc58ce17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      final_filtered|            features|\n",
      "+--------------------+--------------------+\n",
      "|[purpose, study, ...|(10000,[1072,1241...|\n",
      "|[paper, describes...|(10000,[110,157,2...|\n",
      "|[article, applied...|(10000,[45,188,79...|\n",
      "|[recent, achievem...|(10000,[86,157,18...|\n",
      "|[recently, bridge...|(10000,[107,310,4...|\n",
      "|[applications, ab...|(10000,[592,764,8...|\n",
      "|[three, speech, t...|(10000,[39,72,102...|\n",
      "|[paper, focuses, ...|(10000,[157,749,1...|\n",
      "|[embedded, system...|(10000,[7,364,573...|\n",
      "|[xax, browser, pl...|(10000,[23,286,38...|\n",
      "|[recent, years, m...|(10000,[157,274,3...|\n",
      "|[previous, langua...|(10000,[134,221,4...|\n",
      "|[spatial, encrypt...|(10000,[46,270,27...|\n",
      "|[system, operatio...|(10000,[7,15,163,...|\n",
      "|[business, strate...|(10000,[120,157,2...|\n",
      "|[ftp, mirror, tra...|(10000,[42,78,193...|\n",
      "|[number, alternat...|(10000,[1020,1137...|\n",
      "|[breast, cancer, ...|(10000,[157,274,4...|\n",
      "|[development, aut...|(10000,[23,32,45,...|\n",
      "|[quality, specifi...|(10000,[279,293,3...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the final DataFrame with TF-IDF features\n",
    "tfidf_df.select(\"final_filtered\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4fde4-9ba3-4356-95ee-4386adb6bd1e",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a101745-6a2b-4043-8fab-d0595bd4fdbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o151.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat breeze.linalg.svd$.breeze$linalg$svd$$doSVD_Double(svd.scala:94)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:36)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:35)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.svd$.apply(svd.scala:21)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:501)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#throws somekind of error\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimited_tfidf_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m pca_df \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(limited_tfidf_df)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o151.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat breeze.linalg.svd$.breeze$linalg$svd$$doSVD_Double(svd.scala:94)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:36)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:35)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.svd$.apply(svd.scala:21)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:501)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Select the first 500 rows\n",
    "limited_tfidf_df = tfidf_df.limit(500)\n",
    "\n",
    "#throws somekind of error\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(limited_tfidf_df)\n",
    "pca_df = pca_model.transform(limited_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0722787d-e7b2-48cd-a59b-de8258644dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pca_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(k)\n\u001b[1;32m     14\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit(\u001b[43mpca_df\u001b[49m)\n\u001b[1;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(pca_df)\n\u001b[1;32m     17\u001b[0m silhouette \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#without PCA, this \"works\", need to change the featuresCol name and df name\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "cost = []\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "for k in range(2, 11):\n",
    "    print(k)\n",
    "    kmeans = KMeans(featuresCol='pca_features', k=k)\n",
    "    model = kmeans.fit(pca_df)\n",
    "    predictions = model.transform(pca_df)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    cost.append((k, silhouette))\n",
    "    print(f\"With K={k}, the Silhouette score is {silhouette}\")\n",
    "\n",
    "# Choose the best K (you can automate this step)\n",
    "best_k = max(cost, key=lambda item: item[1])[0]\n",
    "print(f\"Best K found: {best_k}\")\n",
    "\n",
    "# Fit the final K-means model with the best K\n",
    "kmeans = KMeans(featuresCol='pca_features', k=best_k)\n",
    "model = kmeans.fit(pca_df)\n",
    "predictions = model.transform(pca_df)\n",
    "\n",
    "# Show the resulting clusters\n",
    "predictions.select(\"id\", \"prediction\").show()\n",
    "\n",
    "# If you want to see the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259e4ed-eb94-4d13-88b4-430c2e5cdc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
