{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316839b1-2c21-413d-bff1-08c9f0f58b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext represents the connection to a Spark cluster\n",
    "from pyspark.context import SparkContext\n",
    "# Configuration for a Spark application\n",
    "from pyspark.conf import SparkConf\n",
    "# The entry point to programming Spark with the Dataset and DataFrame API\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"P03_Clustering\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    #.config(\"spark.sql.repl.eagerEval.truncate\", 500) \\\n",
    "    #.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5458e95a-802e-4cd7-ba14-0083aea1db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dblp_ref_file_path = \"dblp-ref/dblp-ref-0.json\" # This is the first of four files\n",
    "papers_df = spark.read.json(dblp_ref_file_path)\n",
    "\n",
    "papers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f9a210-2378-43f8-bb79-13bf78767c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|                NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|                NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|                NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|                NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|                NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|\n",
      "|                NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|\n",
      "|                NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|\n",
      "|                NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|                NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|                NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6254-b384-456a-9f92-365651aa575a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70043032-86b9-431b-8043-366e6ecc62be",
   "metadata": {},
   "source": [
    "### Required packages and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef225f03-a517-4e63-9f59-cea33a9b1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83015362-a6fc-444a-bc5c-5019959e649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fast-langdetect in /opt/conda/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: fasttext-wheel>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (0.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (2.31.0)\n",
      "Requirement already satisfied: robust-downloader>=0.0.2 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (0.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/conda/lib/python3.11/site-packages (from fast-langdetect) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.11/site-packages (from fasttext-wheel>=0.9.2->fast-langdetect) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from fasttext-wheel>=0.9.2->fast-langdetect) (69.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31.0->fast-langdetect) (2024.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from robust-downloader>=0.0.2->fast-langdetect) (4.66.2)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from robust-downloader>=0.0.2->fast-langdetect) (6.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fast-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbff3da5-a400-476c-85bf-021a4da98483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If you're running the language detection for the first time, try running this if the dataframe is filtered to empty\n",
    "# On first time run it might need to download a small language file that the udf might not trigger\n",
    "from fast_langdetect import detect_langs\n",
    "\n",
    "print(detect_langs(\"Hello, world!\") == 'EN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef45372e-05af-436d-b0a5-0b4685498d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, split, coalesce, array\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from langdetect import detect, LangDetectException\n",
    "from fast_langdetect import detect_langs\n",
    "\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', \n",
    "                     'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return regexp_replace(text, r'[!()\\[\\]{};:\"\\,<>./?@#$%^&*_~]', '')\n",
    "\n",
    "# Language detection (EN)\n",
    "def detect_language(text):\n",
    "    result = False\n",
    "    try:\n",
    "        split_text = text.split(\" \")\n",
    "        if len(split_text) < 9:\n",
    "            result = detect(text) == 'en'\n",
    "        else:\n",
    "            result = \" \".join(split_text[:9]) == 'en'\n",
    "    except LangDetectException:\n",
    "        result = False\n",
    "    return result\n",
    "\n",
    "detect_language_udf = udf(detect_language, BooleanType())\n",
    "\n",
    "# ASCII detection - unused experiment but left it here for reference\n",
    "def detect_ascii(text):\n",
    "    return text.isascii()\n",
    "\n",
    "detect_ascii_udf = udf(detect_ascii, BooleanType())\n",
    "\n",
    "# Fast language detection (EN)\n",
    "def fast_detect_language(text):\n",
    "    result = False\n",
    "    try:\n",
    "        result = detect_langs(text) == 'EN'\n",
    "    except Exception:\n",
    "        result = False\n",
    "    return result\n",
    "\n",
    "fast_detect_language_udf = udf(fast_detect_language, BooleanType())\n",
    "\n",
    "# Custom stop words removal\n",
    "def remove_custom_stop_words(words, custom_stop_words):\n",
    "    if words is None:\n",
    "        return None\n",
    "    return [word for word in words if word not in custom_stop_words]\n",
    "\n",
    "remove_custom_stop_words_udf = udf(lambda words: remove_custom_stop_words(words, custom_stop_words), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2248c-c4db-479d-b09e-7e8d50c7903c",
   "metadata": {},
   "source": [
    "### Set up the required filtering and preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06241d25-9531-464c-baf3-60ef731ca643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out papers that don't have useable abstracts\n",
    "papers_cleaned_df = papers_df.filter(col(\"abstract\").isNotNull() & (col(\"abstract\").rlike(r'\\w')))\n",
    "\n",
    "# Filter out papers that are not in English\n",
    "papers_cleaned_df = papers_cleaned_df.filter(fast_detect_language_udf(col(\"title\")))\n",
    "#papers_cleaned_df = papers_cleaned_df.limit(100) # Useful for testing, takes only the first n entries\n",
    "#papers_cleaned_df = papers_cleaned_df.filter(detect_ascii_udf(col(\"title\"))) # \"Language detection\" based on if the title contains any non-ASCII characters (slow and kind of wrong)\n",
    "#papers_cleaned_df = papers_cleaned_df.filter(detect_language_udf(col(\"title\"))) # Language detection using the base language library (very slow)\n",
    "\n",
    "# Simplify the format of the abstract\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"abstract\", remove_punctuation(col(\"abstract\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"abstract\", lower(col(\"abstract\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"abstract_words\", split(col(\"abstract\"), \" \"))\n",
    "\n",
    "# Conform references\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"references\", coalesce(\"references\", array()))\n",
    "\n",
    "# Preprocess title column\n",
    "papers_cleaned_df = papers_cleaned_df.filter(col(\"title\").isNotNull() & (col(\"title\").rlike(r'\\w')))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"title\", remove_punctuation(col(\"title\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"title\", lower(col(\"title\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"title_words\", split(col(\"title\"), \" \"))\n",
    "\n",
    "# Remove pre-given stop words from the title\n",
    "remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "papers_cleaned_df = remover.transform(papers_cleaned_df)\n",
    "\n",
    "# Remove pre-given stop words from the abstract\n",
    "remover = StopWordsRemover(inputCol=\"abstract_words\", outputCol=\"filtered_abstract_words\")\n",
    "papers_cleaned_df = remover.transform(papers_cleaned_df)\n",
    "\n",
    "# Remove custom stop words from the title and the abstract\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"final_filtered_abstract\", remove_custom_stop_words_udf(col(\"filtered_abstract_words\")))\n",
    "papers_cleaned_df = papers_cleaned_df.withColumn(\"final_filtered_title\", remove_custom_stop_words_udf(col(\"filtered_title_words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8885ffce-90dd-40e6-b7bb-4374fdec6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns that are not used further down the line\n",
    "# Keeps the following:\n",
    "# - id                          (for reference)\n",
    "# - authors                     (used for clustering)\n",
    "# - final_filtered_abstract     (used for clustering)\n",
    "# - final_filtered_title        (used for clustering)\n",
    "columns_to_drop = [\"abstract\", \"abstract_words\", \"title\", \"title_words\", \"filtered_title_words\",\n",
    "                   \"filtered_abstract_words\", \"n_citation\", \"references\", \"venue\", \"year\"]\n",
    "\n",
    "papers_cleaned_df = papers_cleaned_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af405016-9837-4c7b-b70f-3a1dd15b1661",
   "metadata": {},
   "source": [
    "### Run the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3f6e03-bfa6-4bdf-a450-658889929684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resulting papers: 743421\n",
      "\n",
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "|             authors|                  id|final_filtered_abstract|final_filtered_title|\n",
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|   [purpose, study, ...|[preliminary, des...|\n",
      "|[Gareth Beale, Gr...|001c58d3-26ad-46b...|   [paper, describes...|[methodology, phy...|\n",
      "|[Altaf Hossain, F...|001c8744-73c4-4b0...|   [article, applied...|[comparison, garc...|\n",
      "|[Ankita Brahmacha...|00a119c4-d367-460...|   [recent, achievem...|[identifying, psy...|\n",
      "|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|   [recently, bridge...|[multisymplectic,...|\n",
      "|[Patrick Cousot, ...|00c85316-bddf-4bc...|   [applications, ab...|[relational, abst...|\n",
      "|[Minoru Shigenaga...|00ca027b-5174-40f...|   [three, speech, t...|[speech, training...|\n",
      "|[Efthymios Alepis...|00dd5ece-1339-4cb...|   [paper, focuses, ...|[knowledge, engin...|\n",
      "|[Matias Madou, Bj...|01047814-b615-444...|   [embedded, system...|[link-time, compa...|\n",
      "|[John R. Douceur,...|010d4ce9-0279-416...|   [xax, browser, pl...|[leveraging, lega...|\n",
      "|[Hiroshi Furukawa...|010d9907-45ef-459...|   [recent, years, m...|[pedestrian, navi...|\n",
      "|[Carmen Fernández...|012b88ae-a763-45d...|   [previous, langua...|[word, pairs, lan...|\n",
      "|[Michel Abdalla, ...|016a9a21-e882-4cd...|   [spatial, encrypt...|[leakage-resilien...|\n",
      "|[Mark Burgess, Al...|01705f09-d395-4a0...|   [system, operatio...|[system, rollback...|\n",
      "|[Constantinos Gia...|01b6f2ca-3903-419...|   [business, strate...|[model-driven, st...|\n",
      "|[Alexei Novikov, ...|01ccb92f-46f1-400...|   [ftp, mirror, tra...|[ftp, mirror, tra...|\n",
      "|[Theresa Beauboue...|01edeac9-cd8b-46f...|   [number, alternat...|[information, sys...|\n",
      "|[Bartosz Krawczyk...|0265aea8-65f3-4f4...|   [breast, cancer, ...|[breast, cancer, ...|\n",
      "|[J. Niblock, Jian...|028d37c8-b571-41b...|   [development, aut...|[automated, objec...|\n",
      "|[Qiong Liu, You Y...|02a5e8a8-061e-4c1...|   [quality, specifi...|[quality, assessm...|\n",
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "The process took 17.655625343322754 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "print(\"Number of resulting papers:\", papers_cleaned_df.count())\n",
    "\n",
    "print()\n",
    "papers_cleaned_df.show()\n",
    "print()\n",
    "\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"The process took\", (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebe98b-8319-4175-82c4-93ad35fa2c3f",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588a6fd8-3d1a-4ce5-9d49-3b083756ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used throughout the process\n",
    "N_FEATURES_ABSTRACT = 1024\n",
    "N_FEATURES_TITLE = 1024\n",
    "N_FEATURES_AUTHORS = 1024\n",
    "DATA_LIMIT = 100_000 # Set negative to include all of the data\n",
    "PCA_k = 128\n",
    "CLUSTERING_k_RANGE = (3,6)\n",
    "CLUSTERING_k_FINAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7da1058-f2b1-4520-a488-f1ca1faa6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import when\n",
    "import time\n",
    "\n",
    "# NB! This takes a long time to calculate so don't run this often (run time of ~1 minute)\n",
    "\"\"\"hashing_tf = HashingTF(inputCol=\"final_filtered\", outputCol=\"raw_features\", numFeatures=1024)\n",
    "tf_df = hashing_tf.transform(papers_cleaned_df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "tfidf_df = tfidf_df.filter(col(\"features\").isNotNull())\n",
    "\"\"\"\n",
    "\n",
    "# HashingTF for abstract and title\n",
    "hashingTF_abstract = HashingTF(inputCol=\"final_filtered_abstract\", outputCol=\"raw_abstract_features\", numFeatures=N_FEATURES_ABSTRACT)\n",
    "hashingTF_title = HashingTF(inputCol=\"final_filtered_title\", outputCol=\"raw_title_features\", numFeatures=N_FEATURES_TITLE)\n",
    "\n",
    "# IDF for abstract\n",
    "idf_abstract = IDF(inputCol=\"raw_abstract_features\", outputCol=\"abstract_features\")\n",
    "\n",
    "# IDF for title\n",
    "idf_title = IDF(inputCol=\"raw_title_features\", outputCol=\"title_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7852e1e0-dc7b-4eb5-8bc4-abcc58ce17bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingTF for abstract and title took 0.3699822425842285 seconds\n",
      "IDF for abstract took 62.56141018867493 seconds\n",
      "IDF for title took 19.113986253738403 seconds\n",
      "HashingTF for authors took 0.03081822395324707 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "# Apply HashingTF to abstract and title\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "featurized_df = hashingTF_abstract.transform(papers_cleaned_df)\n",
    "featurized_df = hashingTF_title.transform(featurized_df)\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"HashingTF for abstract and title took\", (end - start), \"seconds\")\n",
    "\n",
    "# Apply IDF on abstract\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "rescaled_df = idf_abstract.fit(featurized_df).transform(featurized_df)\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"IDF for abstract took\", (end - start), \"seconds\")\n",
    "\n",
    "# Apply IDF on title\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "rescaled_df = idf_title.fit(rescaled_df).transform(rescaled_df)\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"IDF for title took\", (end - start), \"seconds\")\n",
    "\n",
    "# Normalize n_citation\n",
    "# Skipping this for now as n_citation had an anomaly where ~20% of all data points had the value 50, which did not make sense\n",
    "#assembler = VectorAssembler(inputCols=[\"n_citation\"], outputCol=\"n_citation_vec\")\n",
    "#rescaled_df = assembler.transform(rescaled_df)\n",
    "#scaler = StandardScaler(inputCol=\"n_citation_vec\", outputCol=\"scaled_n_citation\")\n",
    "#rescaled_df = scaler.fit(rescaled_df).transform(rescaled_df)\n",
    "\n",
    "# HashingTF for authors\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "hashingTF_authors = HashingTF(inputCol=\"authors\", outputCol=\"authors_features\", numFeatures=N_FEATURES_AUTHORS)\n",
    "rescaled_df = hashingTF_authors.transform(rescaled_df)\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"HashingTF for authors took\", (end - start), \"seconds\")\n",
    "\n",
    "# Combine all features\n",
    "assembler = VectorAssembler(inputCols=[\"abstract_features\", \"title_features\", \"authors_features\"], outputCol=\"features\") # Removed \"scaled_n_citation\" for now\n",
    "final_df = assembler.transform(rescaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe15ae38-bca3-4a17-b085-38ee0461c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop intermediary columns\n",
    "columns_to_drop = [\"final_filtered_abstract\", \"final_filtered_title\", \"authors\",\n",
    "                   \"raw_abstract_features\", \"raw_title_features\",\n",
    "                   \"abstract_features\", \"title_features\", \"authors_features\" ]\n",
    "\n",
    "final_df = final_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1a884e-9fdd-4a41-b6dd-7c4488ea6f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+--------------------+--------------------+\n",
      "|                  id|            features|\n",
      "+--------------------+--------------------+\n",
      "|00127ee2-cb05-48c...|(3072,[68,73,104,...|\n",
      "|001c58d3-26ad-46b...|(3072,[28,49,59,7...|\n",
      "|001c8744-73c4-4b0...|(3072,[5,7,39,44,...|\n",
      "|00a119c4-d367-460...|(3072,[35,42,58,6...|\n",
      "|00bcf2d5-1592-46b...|(3072,[23,94,100,...|\n",
      "|00c85316-bddf-4bc...|(3072,[4,56,107,1...|\n",
      "|00ca027b-5174-40f...|(3072,[6,7,39,56,...|\n",
      "|00dd5ece-1339-4cb...|(3072,[5,6,16,80,...|\n",
      "|01047814-b615-444...|(3072,[31,38,67,8...|\n",
      "|010d4ce9-0279-416...|(3072,[11,16,30,3...|\n",
      "|010d9907-45ef-459...|(3072,[84,92,109,...|\n",
      "|012b88ae-a763-45d...|(3072,[36,43,56,7...|\n",
      "|016a9a21-e882-4cd...|(3072,[29,31,39,4...|\n",
      "|01705f09-d395-4a0...|(3072,[11,12,15,3...|\n",
      "|01b6f2ca-3903-419...|(3072,[46,76,103,...|\n",
      "|01ccb92f-46f1-400...|(3072,[56,57,58,7...|\n",
      "|01edeac9-cd8b-46f...|(3072,[55,58,71,1...|\n",
      "|0265aea8-65f3-4f4...|(3072,[10,17,20,3...|\n",
      "|028d37c8-b571-41b...|(3072,[24,36,44,9...|\n",
      "|02a5e8a8-061e-4c1...|(3072,[67,123,132...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "The process took 3.372274160385132 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_ms = time.time_ns() / 1_000_000_000\n",
    "\n",
    "print()\n",
    "final_df.show()\n",
    "print()\n",
    "\n",
    "end_ms = time.time_ns() / 1_000_000_000\n",
    "print(\"The process took\", (end_ms - start_ms), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4fde4-9ba3-4356-95ee-4386adb6bd1e",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a101745-6a2b-4043-8fab-d0595bd4fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the dataset to the first 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1_000_000_000\u001b[39m\n\u001b[1;32m     13\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39mPCA_k, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimited_final_dropped_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m pca_df \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(limited_final_dropped_df)\n\u001b[1;32m     17\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1_000_000_000\u001b[39m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "import time\n",
    "\n",
    "# Select the first 1000 rows, change this how you see fit and how much your PC can handle\n",
    "if DATA_LIMIT < 0:\n",
    "    print(\"Taking full dataset\")\n",
    "    limited_final_dropped_df = final_df\n",
    "else:\n",
    "    print(\"Limiting the dataset to the first\", DATA_LIMIT, \"rows\")\n",
    "    limited_final_dropped_df = final_df.limit(DATA_LIMIT)\n",
    "\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "pca = PCA(k=PCA_k, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(limited_final_dropped_df)\n",
    "pca_df = pca_model.transform(limited_final_dropped_df)\n",
    "\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"PCA for features took\", (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521d37-c77b-44e3-83ce-39892cba7816",
   "metadata": {},
   "source": [
    "### Clustering with elbowing\n",
    "Run this only if you want to **determine the best k** and then do **clustering with it**. <br>\n",
    "<font color=red>**NB!** The current implementation is **very** slow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0722787d-e7b2-48cd-a59b-de8258644dd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering for k = 3\n",
      "With K=3, the Silhouette score is 0.059832468082140836\n",
      "Clustering for k = 3 took 78.08387064933777 seconds\n",
      "Clustering for k = 4\n",
      "With K=4, the Silhouette score is -0.07965425721369154\n",
      "Clustering for k = 4 took 85.41783213615417 seconds\n",
      "Clustering for k = 5\n",
      "With K=5, the Silhouette score is -0.12199325453327732\n",
      "Clustering for k = 5 took 83.16690015792847 seconds\n",
      "Best K found: 3\n",
      "Finding the best k in range (3, 6) took 246.67020845413208 seconds\n",
      "+--------------------+----------+\n",
      "|                  id|prediction|\n",
      "+--------------------+----------+\n",
      "|00127ee2-cb05-48c...|         0|\n",
      "|001c58d3-26ad-46b...|         2|\n",
      "|001c8744-73c4-4b0...|         0|\n",
      "|00a119c4-d367-460...|         1|\n",
      "|00bcf2d5-1592-46b...|         0|\n",
      "|00c85316-bddf-4bc...|         0|\n",
      "|00ca027b-5174-40f...|         0|\n",
      "|00dd5ece-1339-4cb...|         0|\n",
      "|01047814-b615-444...|         0|\n",
      "|010d4ce9-0279-416...|         0|\n",
      "|010d9907-45ef-459...|         0|\n",
      "|012b88ae-a763-45d...|         0|\n",
      "|016a9a21-e882-4cd...|         0|\n",
      "|01705f09-d395-4a0...|         0|\n",
      "|01b6f2ca-3903-419...|         0|\n",
      "|01ccb92f-46f1-400...|         0|\n",
      "|01edeac9-cd8b-46f...|         0|\n",
      "|0265aea8-65f3-4f4...|         1|\n",
      "|028d37c8-b571-41b...|         0|\n",
      "|02a5e8a8-061e-4c1...|         1|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Refitting the best k took 78.76113057136536 seconds\n",
      "\n",
      "Cluster Centers: \n",
      "[-2.15696034e+00  8.28202733e-01  9.45935732e-01  5.92499906e-01\n",
      " -9.89667135e-01 -1.54126686e+00 -6.27134879e-01  1.18811423e+00\n",
      " -1.36937079e+00 -6.02040247e-02 -1.91975772e-01 -3.48492705e-01\n",
      "  4.50952764e-01 -3.46115224e-01  1.44683887e+00 -3.50583853e-01\n",
      " -3.98078053e-01  4.23496967e-02 -8.88722540e-02 -2.26910728e-02\n",
      "  5.22233961e-02 -7.02816023e-01 -4.21866703e-01 -3.18942401e-02\n",
      " -1.37706067e-01  2.42095123e-01 -3.68755685e-01 -4.95879421e-01\n",
      "  1.18352438e-01 -3.56219496e-01 -6.60707752e-02  6.84263740e-01\n",
      " -3.84625920e-01 -3.41472970e-01  4.90760491e-01  1.40478297e-01\n",
      "  5.10885487e-01  3.07710268e-01 -3.13190525e-01  8.00167791e-01\n",
      " -1.81008569e-01 -4.41711224e-01 -2.47257853e-01  5.64923912e-01\n",
      "  3.02297180e-01  2.12622239e-01 -7.22480008e-01 -4.52811775e-01\n",
      "  2.94384462e-01  1.39071759e-01 -2.30050415e-01  3.18976826e-01\n",
      " -2.07433512e-01 -1.10292793e-01 -3.44335178e-01  9.33975455e-02\n",
      " -4.05914262e-01  4.63645981e-01  1.88131762e-01  2.90456072e-01\n",
      " -4.31304588e-01 -3.96410507e-01 -4.89463428e-01  4.29485149e-01\n",
      " -7.92844131e-02  1.23527080e-01  2.16003785e-01  1.93110584e-01\n",
      " -2.41171891e-01 -5.94549731e-02 -2.74139103e-01 -1.95158937e-01\n",
      " -1.24445993e-04  3.97877256e-01  1.20511572e-01 -1.15100671e-02\n",
      "  2.69759125e-02 -9.70956782e-02  5.94264208e-01  7.26915740e-02\n",
      "  8.73252920e-02  8.96889226e-02 -3.19996971e-01 -3.08018073e-01\n",
      " -3.15118010e-01  1.32521183e-01  3.70670462e-01 -5.70280852e-01\n",
      " -2.09656980e-01  2.46546125e-02 -4.10980204e-01 -7.51359958e-01\n",
      " -3.45558369e-01  4.85006001e-02 -6.95799422e-02  6.40658453e-01\n",
      " -4.75350329e-01  9.47382104e-01  4.31737639e-01 -1.98312440e-01\n",
      " -3.37833320e-02  7.30835351e-02 -8.89022594e-02  1.50685286e-02\n",
      " -3.24936898e-01 -1.22419851e-01 -1.56179101e-01 -3.05903194e-02\n",
      " -9.70763696e-02  9.22903435e-02  1.65276409e-01 -1.02297931e-01\n",
      " -1.73684637e-01 -7.99359522e-02 -1.25633376e-01 -2.32849793e-02\n",
      " -1.96771161e-01 -2.09453875e-02  3.04144264e-01 -1.83844942e-02\n",
      " -9.59101462e-03  2.28865430e-01 -5.82664178e-02  5.00382317e-02\n",
      " -1.24640114e-01  9.74751258e-02 -5.44591327e-02  1.72374243e-01]\n",
      "[-2.58764012e+00 -8.26491240e-01  2.19090685e-01  1.22659794e+00\n",
      "  1.50936648e+00 -1.93412462e+00  3.55595033e+00  1.10858393e+01\n",
      " -7.95525537e-02 -9.56839918e-02 -2.27295585e+00  3.58437751e+00\n",
      "  2.79107739e+00 -1.02787237e+00 -2.00867013e-01 -3.42540781e+00\n",
      " -3.88651734e+00  8.10612001e+00  7.01726603e-01 -3.67017565e-01\n",
      "  3.44491093e+00 -2.65367353e-02  1.00847690e+00  2.54841189e-01\n",
      "  1.19316076e+00  2.29000211e+00 -2.37258669e+00  1.44626423e+00\n",
      "  2.32770145e+00 -2.63203545e-01  5.64526770e-01  5.89140107e-01\n",
      " -2.31536334e+00  7.38945155e-01  1.55760183e+00  7.26002124e-01\n",
      " -2.85957647e-01  1.58142537e+00  1.09389801e-01  2.91049264e-01\n",
      "  5.95246246e-01 -9.04327467e-01 -2.12740783e+00  9.13444366e-01\n",
      " -1.53007132e+00 -1.08131246e+00 -7.36839824e-02  1.16084020e+00\n",
      " -1.15899953e+00 -1.03832587e+00 -4.67863609e-01  1.39008937e+00\n",
      " -1.47482013e+00 -1.07764671e+00  1.22955750e-01  5.33451487e-01\n",
      " -1.15792110e+00 -2.97523674e-01 -1.30321664e+00  2.25722087e+00\n",
      "  2.11561888e-01  8.84209712e-01 -6.47574971e-01  9.85192535e-01\n",
      "  4.51638948e-01  5.78919044e-01  1.36875760e+00  6.96525621e-01\n",
      "  1.32513217e-01 -9.10095118e-01  4.39112801e-01  1.96728199e-01\n",
      " -3.09645116e-01 -2.73616998e-01 -8.94117537e-01  7.86255547e-01\n",
      "  3.51346221e-02 -7.36338111e-01  4.05657423e-01  1.14530968e+00\n",
      "  3.82357335e-01  9.52722622e-02 -2.92155856e-01 -1.78045829e+00\n",
      "  6.05608045e-03 -5.54529862e-01  4.39743959e-02 -1.36329633e+00\n",
      " -5.94121740e-01  1.35990322e-01 -9.81246070e-01 -5.99981281e-01\n",
      " -3.34796975e-01 -6.22370939e-02 -4.40199064e-01  4.74913431e-01\n",
      " -9.12963038e-01  8.47495372e-01  5.75217204e-01 -1.98312440e-01\n",
      " -3.37833320e-02  7.30835351e-02 -8.89022594e-02  1.50685286e-02\n",
      " -3.24936898e-01 -1.22419851e-01 -1.56179101e-01 -3.05903194e-02\n",
      " -9.70763696e-02  9.22903435e-02  1.65276409e-01 -1.02297931e-01\n",
      " -1.73684637e-01 -7.99359522e-02 -1.25633376e-01 -2.32849793e-02\n",
      " -1.96771161e-01 -2.09453875e-02  3.04144264e-01 -1.83844942e-02\n",
      " -9.59101462e-03  2.28865430e-01 -5.82664178e-02  5.00382317e-02\n",
      " -1.24640114e-01  9.74751258e-02 -5.44591327e-02  1.72374243e-01]\n",
      "[-4.09279434e+00 -1.47312974e+00 -6.14856172e-01  2.22390835e-01\n",
      "  1.04142570e+00 -1.32410598e+00 -9.33825024e-01  2.86024896e+00\n",
      "  1.96383744e-01  2.80418392e+00  3.00789855e+00 -3.40706274e+00\n",
      " -1.92525912e-01  4.77263989e+00 -1.38514956e+00 -7.98147955e-01\n",
      "  1.30653899e-01  1.49811912e+00  6.19291327e-01  5.00637910e+00\n",
      " -4.83915542e+00 -1.13617430e+01 -2.68466225e+00  3.42078368e+00\n",
      " -1.60914797e+00 -1.26032762e+01 -8.10031760e+00 -6.84212601e+00\n",
      "  2.65736657e+00 -6.89468172e+00 -9.29070337e-01 -1.90936115e+00\n",
      " -5.98442348e+00  8.19204135e+00  8.31150892e+00 -5.41055148e-01\n",
      "  3.55938205e+00  1.75783018e+00  4.44730397e+00  4.24138809e+00\n",
      " -9.07359628e+00  3.38361361e+00 -2.98519890e-01 -1.85009142e+00\n",
      " -3.20478718e+00  5.33120301e-01  3.88067656e+00 -1.05619447e+00\n",
      " -1.20999034e+00  6.64215940e-01  4.70728532e-01  9.96176108e-01\n",
      "  6.48262505e-01 -4.85209825e+00  2.38139755e+00  3.95641353e-01\n",
      " -1.98718081e-01  1.88241069e+00 -9.66966368e-01 -7.98570810e-01\n",
      " -3.07707917e-01  1.24519568e+00 -1.83690811e+00 -1.20559311e+00\n",
      "  1.79806145e+00 -4.14691426e-01  6.46676850e-01 -1.11593009e-01\n",
      " -1.49390010e+00  7.40528599e-01  6.27968952e-01 -2.17294147e+00\n",
      "  3.14765556e-01  7.05295759e-01 -1.13501583e+00 -1.19348420e+00\n",
      " -1.09504509e+00  1.20861707e-01 -1.42838826e-02  1.47280542e+00\n",
      " -7.23418375e-01 -1.84489532e+00 -3.93653223e-01 -2.74458620e-01\n",
      " -9.26826071e-01 -2.97078927e-01  6.46132476e-01  4.25990334e-01\n",
      " -5.13715813e-01  3.30103069e-01 -3.83731709e-01 -1.15183707e+00\n",
      "  1.15169171e-01  4.10168481e-01 -5.42313767e-01  2.20181078e-01\n",
      " -8.74198761e-01  9.50408351e-01  5.52178863e-01 -1.98312440e-01\n",
      " -3.37833320e-02  7.30835351e-02 -8.89022594e-02  1.50685286e-02\n",
      " -3.24936898e-01 -1.22419851e-01 -1.56179101e-01 -3.05903194e-02\n",
      " -9.70763696e-02  9.22903435e-02  1.65276409e-01 -1.02297931e-01\n",
      " -1.73684637e-01 -7.99359522e-02 -1.25633376e-01 -2.32849793e-02\n",
      " -1.96771161e-01 -2.09453875e-02  3.04144264e-01 -1.83844942e-02\n",
      " -9.59101462e-03  2.28865430e-01 -5.82664178e-02  5.00382317e-02\n",
      " -1.24640114e-01  9.74751258e-02 -5.44591327e-02  1.72374243e-01]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import time\n",
    "\n",
    "# NB! This process is currently extremely slow\n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "cost = []\n",
    "evaluator = ClusteringEvaluator()\n",
    "start_det = time.time_ns() / 1_000_000_000\n",
    "\n",
    "for k in range(CLUSTERING_k_RANGE[0], CLUSTERING_k_RANGE[1]):\n",
    "    start = time.time_ns() / 1_000_000_000\n",
    "    print(\"Clustering for k =\", k)\n",
    "    kmeans = KMeans(featuresCol='pca_features', k=k)\n",
    "    model = kmeans.fit(pca_df)\n",
    "    predictions = model.transform(pca_df)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    cost.append((k, silhouette))\n",
    "    print(f\"With K={k}, the Silhouette score is {silhouette}\")\n",
    "    end = time.time_ns() / 1_000_000_000\n",
    "    print(\"Clustering for k =\", k, \"took\", (end - start), \"seconds\")\n",
    "\n",
    "# Choose the best K (you can automate this step)\n",
    "best_k = max(cost, key=lambda item: item[1])[0]\n",
    "print(f\"Best K found: {best_k}\")\n",
    "end_det = time.time_ns() / 1_000_000_000\n",
    "print(\"Finding the best k in range\", CLUSTERING_k_RANGE, \"took\", (end_det - start_det), \"seconds\")\n",
    "\n",
    "# Fit the final K-means model with the best K\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "kmeans = KMeans(featuresCol='pca_features', k=best_k)\n",
    "model = kmeans.fit(pca_df)\n",
    "predictions = model.transform(pca_df)\n",
    "\n",
    "# Show the resulting clusters\n",
    "predictions.select(\"id\", \"prediction\").show()\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"\\nRefitting the best k took\", (end - start), \"seconds\\n\")\n",
    "\n",
    "# If you want to see the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8419c-0766-47cf-ab8e-5c52000303b7",
   "metadata": {},
   "source": [
    "### Clustering with predefined k\n",
    "Run this only if you want to **cluster with the predefined k**. <br>\n",
    "<font color=red>**NB!** The current implementation is slow, but less so than the previous cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d8519-dc7c-4286-bdd7-e11de1ce6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Fit the final K-means model with the predefined k\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "kmeans = KMeans(featuresCol='pca_features', k=CLUSTERING_k_FINAL)\n",
    "model = kmeans.fit(pca_df)\n",
    "predictions = model.transform(pca_df)\n",
    "end = time.time_ns() / 1_000_000_000\n",
    "print(\"Clustering for k =\", CLUSTERING_k_FINAL, \"took\", (end - start), \"seconds\")\n",
    "\n",
    "# Show the resulting clusters\n",
    "predictions.select(\"id\", \"prediction\").show()\n",
    "\n",
    "# Show the silhouette score\n",
    "start = time.time_ns() / 1_000_000_000\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"\\nSilhouette score is\", silhouette)\n",
    "print(\"Calculating the silhouette score took\", (end - start), \"seconds\")\n",
    "\n",
    "# If you want to see the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f327f0d-4b3d-4797-9479-4e0d68321bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"id\", \"prediction\").groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ed6cc-c848-4853-9e97-2f92426b6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB! This overwrites the previous file if any exists\n",
    "predictions.select(\"id\", \"prediction\").write.mode('overwrite').format('json').save('prediction/prediction.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
